{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "from torchdata.datapipes.iter import FileOpener, IterableWrapper\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "import nltk\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output   \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "print('hello')\n",
    "\n",
    "def read_text_file(root: str):\n",
    "    text_dp = IterableWrapper([root])\n",
    "    text_dp = FileOpener(text_dp, mode=\"r\")\n",
    "    return text_dp.readlines(strip_newline=False, return_path=False).shuffle().set_shuffle(False).sharding_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def load_text_file(file_path: str) -> str:\n",
    "    \"\"\"Loads the text from a file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Assuming you have a folder with text files\n",
    "text_files = [os.path.join('./text_folder', f) for f in os.listdir('./text_folder')]\n",
    "raw_text = [read_text_file(f) for f in text_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "=\n",
      ".\n",
      "type\n",
      "language\n",
      "none\n",
      "invalid\n",
      ",\n",
      "format\n",
      "legality\n",
      "legal\n",
      "text\n",
      "name\n",
      "the\n",
      "flavor\n",
      "?\n",
      "wizards\n",
      "http\n",
      "//gatherer\n",
      "ashx\n",
      "com/handlers/image\n",
      "multiverseid\n",
      "a\n",
      "imageurl\n",
      "de\n",
      "creature\n",
      "you\n",
      "of\n",
      "'\n",
      "—\n",
      "to\n",
      ")\n",
      "(\n",
      "it\n",
      "date\n",
      "la\n",
      "if\n",
      "that\n",
      "card\n",
      "hand\n",
      "your\n",
      "and\n",
      "in\n",
      "life\n",
      "criatura\n",
      "or\n",
      "que\n",
      "is\n",
      "number\n",
      "power\n",
      "un\n",
      "0\n",
      "as\n",
      "toughness\n",
      "commander\n",
      "set\n",
      "source\n",
      "ability\n",
      "colors\n",
      "subtypes\n",
      "names\n",
      "supertypes\n",
      "timeshifted\n",
      "starter\n",
      "artist\n",
      "variations\n",
      "normal\n",
      "rulings\n",
      "watermark\n",
      "border\n",
      "rarity\n",
      "cmc\n",
      "color_identity\n",
      "foreign_names\n",
      "id\n",
      "image_url\n",
      "layout\n",
      "legalities\n",
      "loyality\n",
      "mana_cost\n",
      "multiverse_id\n",
      "original_text\n",
      "original_type\n",
      "printings\n",
      "release_date\n",
      "reserved\n",
      "set_name\n",
      "this\n",
      "target\n",
      "on\n",
      "with\n",
      "vintage\n",
      "legacy\n",
      "duel\n",
      "battlefield\n",
      "an\n",
      "s\n",
      "control\n",
      "for\n",
      "spell\n",
      "mana\n",
      "2\n",
      "le\n",
      "di\n",
      "una\n",
      "chinese\n",
      "turn\n",
      "o\n",
      "its\n",
      "cast\n",
      "die\n",
      "du\n",
      "1\n",
      "player\n",
      "no\n",
      "be\n",
      "any\n",
      "{t}\n",
      "creatura\n",
      "modern\n",
      "from\n",
      "créature\n",
      "3\n",
      "can\n",
      "+1/+1\n",
      "put\n",
      "when\n",
      "kreatur\n",
      "japanese\n",
      "cards\n",
      "french\n",
      "german\n",
      "italian\n",
      "spanish\n",
      "penny\n",
      "damage\n",
      "à\n",
      "el\n",
      "may\n",
      "will\n",
      "cost\n",
      "brazil\n",
      "portuguese\n",
      "del\n",
      "do\n",
      "t\n",
      "each\n",
      "il\n",
      "creatures\n",
      "der\n",
      "one\n",
      "paupercommander\n",
      "at\n",
      "eine\n",
      "se\n",
      "des\n",
      "turno\n",
      "e\n",
      "simplified\n",
      "has\n",
      "um\n",
      "4\n",
      "en\n",
      "vous\n",
      "have\n",
      "b\n",
      "end\n",
      "are\n",
      "w\n",
      "u\n",
      "r\n",
      "g\n",
      "enters\n",
      "graveyard\n",
      "until\n",
      "russian\n",
      "et\n",
      "campo\n",
      "all\n",
      "land\n",
      "carta\n",
      "artifact\n",
      "into\n",
      "x\n",
      "l\n",
      "pioneer\n",
      "by\n",
      "une\n",
      "whenever\n",
      "les\n",
      "not\n",
      "choose\n",
      "counter\n",
      "common\n",
      "only\n",
      "pauper\n",
      "other\n",
      "abilities\n",
      "deiner\n",
      "você\n",
      "permanent\n",
      "human\n",
      "und\n",
      "library\n",
      "time\n",
      "sorcery\n",
      "si\n",
      "traditional\n",
      "rare\n",
      "da\n",
      "aura\n",
      "instant\n",
      "counters\n",
      "uncommon\n",
      "che\n",
      "historic\n",
      "then\n",
      "ou\n",
      "carte\n",
      "gladiator\n",
      "historicbrawl\n",
      "two\n",
      "flying\n",
      "con\n",
      "votre\n",
      "pay\n",
      "enchantment\n",
      "d\n",
      "tu\n",
      "al\n",
      "effect\n",
      "zu\n",
      "wenn\n",
      "{1}\n",
      "quando\n",
      "korean\n",
      "deals\n",
      "gets\n",
      "sacrifice\n",
      "exile\n",
      "para\n",
      "combat\n",
      "uma\n",
      "y\n",
      "wahl\n",
      "5\n",
      "opponent\n",
      "{2}\n",
      "explorer\n",
      "their\n",
      "auf\n",
      "legendary\n",
      "resolves\n",
      "more\n",
      "seu\n",
      "draw\n",
      "trigger\n",
      "los\n",
      "token\n",
      "they\n",
      "sur\n",
      "bersaglio\n",
      "objetivo\n",
      "ins\n",
      "i\n",
      "spiel\n",
      "alvo\n",
      "won’t\n",
      "final\n",
      "face\n",
      "premodern\n",
      "su\n",
      "-\n",
      "«\n",
      "karte\n",
      "»\n",
      "can’t\n",
      "em\n",
      "was\n",
      "first\n",
      "non\n",
      "game\n",
      "sie\n",
      "cada\n",
      "than\n",
      "chaque\n",
      "entra\n",
      "copy\n",
      "esta\n",
      "gain\n",
      "play\n",
      "up\n",
      "them\n",
      "planeswalker\n",
      "before\n",
      "ein\n",
      "return\n",
      "color\n",
      "doesn’t\n",
      "those\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, raw_text[1]), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "for x in range(0,300):\n",
    "    print(vocab.lookup_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filehandler = open(\"vocab.obj\",\"wb\")\n",
    "pickle.dump(vocab,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_iter = raw_text[1]\n",
    "val_iter = raw_text[2]\n",
    "test_iter = raw_text[0]\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/28904 batches | lr 5.00 | ms/batch 1983.64 | loss  6.87 | ppl   963.87\n",
      "| epoch   1 |   400/28904 batches | lr 5.00 | ms/batch 1977.75 | loss  4.41 | ppl    81.97\n",
      "| epoch   1 |   600/28904 batches | lr 5.00 | ms/batch 1954.68 | loss  3.82 | ppl    45.74\n",
      "| epoch   1 |   800/28904 batches | lr 5.00 | ms/batch 1912.61 | loss  3.52 | ppl    33.72\n",
      "| epoch   1 |  1000/28904 batches | lr 5.00 | ms/batch 1907.72 | loss  3.27 | ppl    26.34\n",
      "| epoch   1 |  1200/28904 batches | lr 5.00 | ms/batch 1908.52 | loss  3.07 | ppl    21.45\n",
      "| epoch   1 |  1400/28904 batches | lr 5.00 | ms/batch 1909.52 | loss  3.03 | ppl    20.64\n",
      "| epoch   1 |  1600/28904 batches | lr 5.00 | ms/batch 1910.73 | loss  2.82 | ppl    16.84\n",
      "| epoch   1 |  1800/28904 batches | lr 5.00 | ms/batch 2062.50 | loss  2.74 | ppl    15.44\n",
      "| epoch   1 |  2000/28904 batches | lr 5.00 | ms/batch 2107.70 | loss  2.85 | ppl    17.27\n",
      "| epoch   1 |  2200/28904 batches | lr 5.00 | ms/batch 2054.78 | loss  2.72 | ppl    15.22\n",
      "| epoch   1 |  2400/28904 batches | lr 5.00 | ms/batch 2014.16 | loss  2.69 | ppl    14.73\n",
      "| epoch   1 |  2600/28904 batches | lr 5.00 | ms/batch 2021.77 | loss  2.66 | ppl    14.29\n",
      "| epoch   1 |  2800/28904 batches | lr 5.00 | ms/batch 2019.95 | loss  2.70 | ppl    14.85\n",
      "| epoch   1 |  3000/28904 batches | lr 5.00 | ms/batch 2033.43 | loss  2.64 | ppl    13.99\n",
      "| epoch   1 |  3200/28904 batches | lr 5.00 | ms/batch 2028.71 | loss  2.77 | ppl    15.90\n",
      "| epoch   1 |  3400/28904 batches | lr 5.00 | ms/batch 2022.99 | loss  2.57 | ppl    13.10\n",
      "| epoch   1 |  3600/28904 batches | lr 5.00 | ms/batch 2014.07 | loss  2.55 | ppl    12.81\n",
      "| epoch   1 |  3800/28904 batches | lr 5.00 | ms/batch 2010.90 | loss  2.60 | ppl    13.46\n",
      "| epoch   1 |  4000/28904 batches | lr 5.00 | ms/batch 1994.94 | loss  2.53 | ppl    12.60\n",
      "| epoch   1 |  4200/28904 batches | lr 5.00 | ms/batch 2005.40 | loss  2.52 | ppl    12.37\n",
      "| epoch   1 |  4400/28904 batches | lr 5.00 | ms/batch 2020.08 | loss  2.46 | ppl    11.73\n",
      "| epoch   1 |  4600/28904 batches | lr 5.00 | ms/batch 2028.35 | loss  2.48 | ppl    11.99\n",
      "| epoch   1 |  4800/28904 batches | lr 5.00 | ms/batch 2270.95 | loss  2.48 | ppl    11.93\n",
      "| epoch   1 |  5000/28904 batches | lr 5.00 | ms/batch 2195.89 | loss  2.39 | ppl    10.86\n",
      "| epoch   1 |  5200/28904 batches | lr 5.00 | ms/batch 2036.24 | loss  2.44 | ppl    11.48\n",
      "| epoch   1 |  5400/28904 batches | lr 5.00 | ms/batch 2123.41 | loss  2.44 | ppl    11.43\n",
      "| epoch   1 |  5600/28904 batches | lr 5.00 | ms/batch 2106.77 | loss  2.39 | ppl    10.92\n",
      "| epoch   1 |  5800/28904 batches | lr 5.00 | ms/batch 1994.11 | loss  2.47 | ppl    11.79\n",
      "| epoch   1 |  6000/28904 batches | lr 5.00 | ms/batch 1988.78 | loss  2.43 | ppl    11.35\n",
      "| epoch   1 |  6200/28904 batches | lr 5.00 | ms/batch 1980.79 | loss  2.42 | ppl    11.29\n",
      "| epoch   1 |  6400/28904 batches | lr 5.00 | ms/batch 1970.66 | loss  2.38 | ppl    10.75\n",
      "| epoch   1 |  6600/28904 batches | lr 5.00 | ms/batch 1962.42 | loss  2.39 | ppl    10.92\n",
      "| epoch   1 |  6800/28904 batches | lr 5.00 | ms/batch 1969.79 | loss  2.33 | ppl    10.28\n",
      "| epoch   1 |  7000/28904 batches | lr 5.00 | ms/batch 1972.42 | loss  2.43 | ppl    11.40\n",
      "| epoch   1 |  7200/28904 batches | lr 5.00 | ms/batch 1972.81 | loss  2.41 | ppl    11.19\n",
      "| epoch   1 |  7400/28904 batches | lr 5.00 | ms/batch 1974.79 | loss  2.38 | ppl    10.80\n",
      "| epoch   1 |  7600/28904 batches | lr 5.00 | ms/batch 1974.91 | loss  2.32 | ppl    10.18\n",
      "| epoch   1 |  7800/28904 batches | lr 5.00 | ms/batch 1972.38 | loss  2.33 | ppl    10.25\n",
      "| epoch   1 |  8000/28904 batches | lr 5.00 | ms/batch 1967.78 | loss  2.29 | ppl     9.89\n",
      "| epoch   1 |  8200/28904 batches | lr 5.00 | ms/batch 1968.98 | loss  2.35 | ppl    10.53\n",
      "| epoch   1 |  8400/28904 batches | lr 5.00 | ms/batch 1967.89 | loss  2.37 | ppl    10.70\n",
      "| epoch   1 |  8600/28904 batches | lr 5.00 | ms/batch 1975.72 | loss  2.40 | ppl    11.02\n",
      "| epoch   1 |  8800/28904 batches | lr 5.00 | ms/batch 1972.15 | loss  2.38 | ppl    10.84\n",
      "| epoch   1 |  9000/28904 batches | lr 5.00 | ms/batch 1972.44 | loss  2.37 | ppl    10.67\n",
      "| epoch   1 |  9200/28904 batches | lr 5.00 | ms/batch 1972.95 | loss  2.32 | ppl    10.15\n",
      "| epoch   1 |  9400/28904 batches | lr 5.00 | ms/batch 1971.62 | loss  2.37 | ppl    10.67\n",
      "| epoch   1 |  9600/28904 batches | lr 5.00 | ms/batch 1966.04 | loss  2.31 | ppl    10.09\n",
      "| epoch   1 |  9800/28904 batches | lr 5.00 | ms/batch 1981.95 | loss  2.33 | ppl    10.30\n",
      "| epoch   1 | 10000/28904 batches | lr 5.00 | ms/batch 1985.04 | loss  2.30 | ppl    10.01\n",
      "| epoch   1 | 10200/28904 batches | lr 5.00 | ms/batch 1986.96 | loss  2.27 | ppl     9.66\n",
      "| epoch   1 | 10400/28904 batches | lr 5.00 | ms/batch 1972.81 | loss  2.31 | ppl    10.05\n",
      "| epoch   1 | 10600/28904 batches | lr 5.00 | ms/batch 1964.27 | loss  2.26 | ppl     9.60\n",
      "| epoch   1 | 10800/28904 batches | lr 5.00 | ms/batch 1963.09 | loss  2.32 | ppl    10.13\n",
      "| epoch   1 | 11000/28904 batches | lr 5.00 | ms/batch 1958.86 | loss  2.41 | ppl    11.17\n",
      "| epoch   1 | 11200/28904 batches | lr 5.00 | ms/batch 1958.33 | loss  2.31 | ppl    10.12\n",
      "| epoch   1 | 11400/28904 batches | lr 5.00 | ms/batch 1972.00 | loss  2.29 | ppl     9.84\n",
      "| epoch   1 | 11600/28904 batches | lr 5.00 | ms/batch 1981.57 | loss  2.26 | ppl     9.56\n",
      "| epoch   1 | 11800/28904 batches | lr 5.00 | ms/batch 1987.65 | loss  2.37 | ppl    10.68\n",
      "| epoch   1 | 12000/28904 batches | lr 5.00 | ms/batch 1974.73 | loss  2.36 | ppl    10.57\n",
      "| epoch   1 | 12200/28904 batches | lr 5.00 | ms/batch 1974.28 | loss  2.28 | ppl     9.74\n",
      "| epoch   1 | 12400/28904 batches | lr 5.00 | ms/batch 1964.52 | loss  2.23 | ppl     9.33\n",
      "| epoch   1 | 12600/28904 batches | lr 5.00 | ms/batch 1964.81 | loss  2.33 | ppl    10.32\n",
      "| epoch   1 | 12800/28904 batches | lr 5.00 | ms/batch 1966.55 | loss  2.24 | ppl     9.42\n",
      "| epoch   1 | 13000/28904 batches | lr 5.00 | ms/batch 1964.07 | loss  2.30 | ppl     9.96\n",
      "| epoch   1 | 13200/28904 batches | lr 5.00 | ms/batch 1966.90 | loss  2.24 | ppl     9.41\n",
      "| epoch   1 | 13400/28904 batches | lr 5.00 | ms/batch 1963.07 | loss  2.24 | ppl     9.40\n",
      "| epoch   1 | 13600/28904 batches | lr 5.00 | ms/batch 1962.04 | loss  2.32 | ppl    10.14\n",
      "| epoch   1 | 13800/28904 batches | lr 5.00 | ms/batch 1960.89 | loss  2.34 | ppl    10.34\n",
      "| epoch   1 | 14000/28904 batches | lr 5.00 | ms/batch 1959.83 | loss  2.37 | ppl    10.68\n",
      "| epoch   1 | 14200/28904 batches | lr 5.00 | ms/batch 1964.21 | loss  2.38 | ppl    10.84\n",
      "| epoch   1 | 14400/28904 batches | lr 5.00 | ms/batch 1973.99 | loss  2.36 | ppl    10.56\n",
      "| epoch   1 | 14600/28904 batches | lr 5.00 | ms/batch 1965.51 | loss  2.21 | ppl     9.10\n",
      "| epoch   1 | 14800/28904 batches | lr 5.00 | ms/batch 1957.09 | loss  2.24 | ppl     9.38\n",
      "| epoch   1 | 15000/28904 batches | lr 5.00 | ms/batch 1957.64 | loss  2.36 | ppl    10.64\n",
      "| epoch   1 | 15200/28904 batches | lr 5.00 | ms/batch 1961.74 | loss  2.27 | ppl     9.69\n",
      "| epoch   1 | 15400/28904 batches | lr 5.00 | ms/batch 1957.26 | loss  2.26 | ppl     9.60\n",
      "| epoch   1 | 15600/28904 batches | lr 5.00 | ms/batch 1955.78 | loss  2.27 | ppl     9.70\n",
      "| epoch   1 | 15800/28904 batches | lr 5.00 | ms/batch 1957.53 | loss  2.32 | ppl    10.18\n",
      "| epoch   1 | 16000/28904 batches | lr 5.00 | ms/batch 1947.29 | loss  2.26 | ppl     9.62\n",
      "| epoch   1 | 16200/28904 batches | lr 5.00 | ms/batch 1949.56 | loss  2.27 | ppl     9.66\n",
      "| epoch   1 | 16400/28904 batches | lr 5.00 | ms/batch 1946.06 | loss  2.26 | ppl     9.57\n",
      "| epoch   1 | 16600/28904 batches | lr 5.00 | ms/batch 1948.21 | loss  2.24 | ppl     9.42\n",
      "| epoch   1 | 16800/28904 batches | lr 5.00 | ms/batch 1944.07 | loss  2.36 | ppl    10.61\n",
      "| epoch   1 | 17000/28904 batches | lr 5.00 | ms/batch 1944.38 | loss  2.26 | ppl     9.56\n",
      "| epoch   1 | 17200/28904 batches | lr 5.00 | ms/batch 1944.60 | loss  2.27 | ppl     9.70\n",
      "| epoch   1 | 17400/28904 batches | lr 5.00 | ms/batch 1945.05 | loss  2.31 | ppl    10.03\n",
      "| epoch   1 | 17600/28904 batches | lr 5.00 | ms/batch 1947.28 | loss  2.25 | ppl     9.53\n",
      "| epoch   1 | 17800/28904 batches | lr 5.00 | ms/batch 1946.93 | loss  2.25 | ppl     9.46\n",
      "| epoch   1 | 18000/28904 batches | lr 5.00 | ms/batch 1946.26 | loss  2.23 | ppl     9.32\n",
      "| epoch   1 | 18200/28904 batches | lr 5.00 | ms/batch 1940.59 | loss  2.23 | ppl     9.27\n",
      "| epoch   1 | 18400/28904 batches | lr 5.00 | ms/batch 1941.23 | loss  2.30 | ppl    10.01\n",
      "| epoch   1 | 18600/28904 batches | lr 5.00 | ms/batch 1938.49 | loss  2.28 | ppl     9.75\n",
      "| epoch   1 | 18800/28904 batches | lr 5.00 | ms/batch 1937.70 | loss  2.23 | ppl     9.26\n",
      "| epoch   1 | 19000/28904 batches | lr 5.00 | ms/batch 1940.04 | loss  2.28 | ppl     9.82\n",
      "| epoch   1 | 19200/28904 batches | lr 5.00 | ms/batch 1937.49 | loss  2.49 | ppl    12.06\n",
      "| epoch   1 | 19400/28904 batches | lr 5.00 | ms/batch 1946.28 | loss  2.26 | ppl     9.54\n",
      "| epoch   1 | 19600/28904 batches | lr 5.00 | ms/batch 1938.77 | loss  2.24 | ppl     9.39\n",
      "| epoch   1 | 19800/28904 batches | lr 5.00 | ms/batch 1940.74 | loss  2.19 | ppl     8.91\n",
      "| epoch   1 | 20000/28904 batches | lr 5.00 | ms/batch 1936.71 | loss  2.27 | ppl     9.71\n",
      "| epoch   1 | 20200/28904 batches | lr 5.00 | ms/batch 1938.05 | loss  2.34 | ppl    10.38\n",
      "| epoch   1 | 20400/28904 batches | lr 5.00 | ms/batch 1938.85 | loss  2.32 | ppl    10.20\n",
      "| epoch   1 | 20600/28904 batches | lr 5.00 | ms/batch 1941.03 | loss  2.27 | ppl     9.69\n",
      "| epoch   1 | 20800/28904 batches | lr 5.00 | ms/batch 1933.81 | loss  2.34 | ppl    10.42\n",
      "| epoch   1 | 21000/28904 batches | lr 5.00 | ms/batch 1930.59 | loss  2.29 | ppl     9.84\n",
      "| epoch   1 | 21200/28904 batches | lr 5.00 | ms/batch 1932.19 | loss  2.29 | ppl     9.90\n",
      "| epoch   1 | 21400/28904 batches | lr 5.00 | ms/batch 1938.64 | loss  2.25 | ppl     9.45\n",
      "| epoch   1 | 21600/28904 batches | lr 5.00 | ms/batch 1938.99 | loss  2.27 | ppl     9.69\n",
      "| epoch   1 | 21800/28904 batches | lr 5.00 | ms/batch 1938.92 | loss  2.33 | ppl    10.27\n",
      "| epoch   1 | 22000/28904 batches | lr 5.00 | ms/batch 1943.48 | loss  2.33 | ppl    10.26\n",
      "| epoch   1 | 22200/28904 batches | lr 5.00 | ms/batch 1935.96 | loss  2.32 | ppl    10.22\n",
      "| epoch   1 | 22400/28904 batches | lr 5.00 | ms/batch 1934.69 | loss  2.28 | ppl     9.74\n",
      "| epoch   1 | 22600/28904 batches | lr 5.00 | ms/batch 1925.85 | loss  2.21 | ppl     9.11\n",
      "| epoch   1 | 22800/28904 batches | lr 5.00 | ms/batch 1929.10 | loss  2.30 | ppl     9.94\n",
      "| epoch   1 | 23000/28904 batches | lr 5.00 | ms/batch 1928.31 | loss  2.28 | ppl     9.82\n",
      "| epoch   1 | 23200/28904 batches | lr 5.00 | ms/batch 1932.22 | loss  2.26 | ppl     9.55\n",
      "| epoch   1 | 23400/28904 batches | lr 5.00 | ms/batch 1933.03 | loss  2.27 | ppl     9.71\n",
      "| epoch   1 | 23600/28904 batches | lr 5.00 | ms/batch 1934.25 | loss  2.43 | ppl    11.34\n",
      "| epoch   1 | 23800/28904 batches | lr 5.00 | ms/batch 1931.82 | loss  2.27 | ppl     9.69\n",
      "| epoch   1 | 24000/28904 batches | lr 5.00 | ms/batch 1934.33 | loss  2.30 | ppl     9.95\n",
      "| epoch   1 | 24200/28904 batches | lr 5.00 | ms/batch 1928.27 | loss  2.28 | ppl     9.75\n",
      "| epoch   1 | 24400/28904 batches | lr 5.00 | ms/batch 1931.28 | loss  2.28 | ppl     9.82\n",
      "| epoch   1 | 24600/28904 batches | lr 5.00 | ms/batch 1931.24 | loss  2.21 | ppl     9.14\n",
      "| epoch   1 | 24800/28904 batches | lr 5.00 | ms/batch 1935.37 | loss  2.19 | ppl     8.90\n",
      "| epoch   1 | 25000/28904 batches | lr 5.00 | ms/batch 1929.86 | loss  2.23 | ppl     9.30\n",
      "| epoch   1 | 25200/28904 batches | lr 5.00 | ms/batch 1932.44 | loss  2.19 | ppl     8.89\n",
      "| epoch   1 | 25400/28904 batches | lr 5.00 | ms/batch 1927.71 | loss  2.25 | ppl     9.53\n",
      "| epoch   1 | 25600/28904 batches | lr 5.00 | ms/batch 1928.09 | loss  2.23 | ppl     9.26\n",
      "| epoch   1 | 25800/28904 batches | lr 5.00 | ms/batch 1942.68 | loss  2.19 | ppl     8.95\n",
      "| epoch   1 | 26000/28904 batches | lr 5.00 | ms/batch 1933.63 | loss  2.28 | ppl     9.79\n",
      "| epoch   1 | 26200/28904 batches | lr 5.00 | ms/batch 1931.46 | loss  2.21 | ppl     9.09\n",
      "| epoch   1 | 26400/28904 batches | lr 5.00 | ms/batch 1928.76 | loss  2.24 | ppl     9.43\n",
      "| epoch   1 | 26600/28904 batches | lr 5.00 | ms/batch 1934.60 | loss  2.24 | ppl     9.42\n",
      "| epoch   1 | 26800/28904 batches | lr 5.00 | ms/batch 1924.58 | loss  2.25 | ppl     9.49\n",
      "| epoch   1 | 27000/28904 batches | lr 5.00 | ms/batch 1929.97 | loss  2.19 | ppl     8.95\n",
      "| epoch   1 | 27200/28904 batches | lr 5.00 | ms/batch 1936.11 | loss  2.19 | ppl     8.93\n",
      "| epoch   1 | 27400/28904 batches | lr 5.00 | ms/batch 1930.77 | loss  2.20 | ppl     8.99\n",
      "| epoch   1 | 27600/28904 batches | lr 5.00 | ms/batch 1934.51 | loss  2.24 | ppl     9.43\n",
      "| epoch   1 | 27800/28904 batches | lr 5.00 | ms/batch 1924.93 | loss  2.15 | ppl     8.57\n",
      "| epoch   1 | 28000/28904 batches | lr 5.00 | ms/batch 1926.80 | loss  2.18 | ppl     8.81\n",
      "| epoch   1 | 28200/28904 batches | lr 5.00 | ms/batch 1954.01 | loss  2.24 | ppl     9.41\n",
      "| epoch   1 | 28400/28904 batches | lr 5.00 | ms/batch 1963.70 | loss  2.21 | ppl     9.15\n",
      "| epoch   1 | 28600/28904 batches | lr 5.00 | ms/batch 1953.14 | loss  2.23 | ppl     9.26\n",
      "| epoch   1 | 28800/28904 batches | lr 5.00 | ms/batch 1967.10 | loss  2.22 | ppl     9.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 59793.74s | valid loss  2.20 | valid ppl     9.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/28904 batches | lr 4.75 | ms/batch 1971.94 | loss  2.22 | ppl     9.16\n",
      "| epoch   2 |   400/28904 batches | lr 4.75 | ms/batch 1959.02 | loss  2.17 | ppl     8.78\n",
      "| epoch   2 |   600/28904 batches | lr 4.75 | ms/batch 1965.07 | loss  2.18 | ppl     8.87\n",
      "| epoch   2 |   800/28904 batches | lr 4.75 | ms/batch 1982.75 | loss  2.30 | ppl     9.97\n",
      "| epoch   2 |  1000/28904 batches | lr 4.75 | ms/batch 1952.77 | loss  2.15 | ppl     8.62\n",
      "| epoch   2 |  1200/28904 batches | lr 4.75 | ms/batch 1957.73 | loss  2.17 | ppl     8.75\n",
      "| epoch   2 |  1400/28904 batches | lr 4.75 | ms/batch 1979.39 | loss  2.19 | ppl     8.96\n",
      "| epoch   2 |  1600/28904 batches | lr 4.75 | ms/batch 1962.71 | loss  2.12 | ppl     8.37\n",
      "| epoch   2 |  1800/28904 batches | lr 4.75 | ms/batch 1944.70 | loss  2.15 | ppl     8.59\n",
      "| epoch   2 |  2000/28904 batches | lr 4.75 | ms/batch 1944.67 | loss  2.26 | ppl     9.57\n",
      "| epoch   2 |  2200/28904 batches | lr 4.75 | ms/batch 1923.29 | loss  2.17 | ppl     8.76\n",
      "| epoch   2 |  2400/28904 batches | lr 4.75 | ms/batch 1922.84 | loss  2.30 | ppl    10.01\n",
      "| epoch   2 |  2600/28904 batches | lr 4.75 | ms/batch 1925.50 | loss  2.18 | ppl     8.87\n",
      "| epoch   2 |  2800/28904 batches | lr 4.75 | ms/batch 1928.83 | loss  2.24 | ppl     9.35\n",
      "| epoch   2 |  3000/28904 batches | lr 4.75 | ms/batch 1957.10 | loss  2.18 | ppl     8.89\n",
      "| epoch   2 |  3200/28904 batches | lr 4.75 | ms/batch 1953.35 | loss  2.25 | ppl     9.51\n",
      "| epoch   2 |  3400/28904 batches | lr 4.75 | ms/batch 1947.02 | loss  2.25 | ppl     9.50\n",
      "| epoch   2 |  3600/28904 batches | lr 4.75 | ms/batch 1935.16 | loss  2.20 | ppl     9.06\n",
      "| epoch   2 |  3800/28904 batches | lr 4.75 | ms/batch 1939.27 | loss  2.24 | ppl     9.39\n",
      "| epoch   2 |  4000/28904 batches | lr 4.75 | ms/batch 1936.44 | loss  2.25 | ppl     9.46\n",
      "| epoch   2 |  4200/28904 batches | lr 4.75 | ms/batch 1925.57 | loss  2.26 | ppl     9.63\n",
      "| epoch   2 |  4400/28904 batches | lr 4.75 | ms/batch 1920.09 | loss  2.19 | ppl     8.94\n",
      "| epoch   2 |  4600/28904 batches | lr 4.75 | ms/batch 1938.34 | loss  2.19 | ppl     8.92\n",
      "| epoch   2 |  4800/28904 batches | lr 4.75 | ms/batch 1935.99 | loss  2.23 | ppl     9.27\n",
      "| epoch   2 |  5000/28904 batches | lr 4.75 | ms/batch 1940.44 | loss  2.15 | ppl     8.62\n",
      "| epoch   2 |  5200/28904 batches | lr 4.75 | ms/batch 1935.34 | loss  2.18 | ppl     8.81\n",
      "| epoch   2 |  5400/28904 batches | lr 4.75 | ms/batch 1931.96 | loss  2.18 | ppl     8.87\n",
      "| epoch   2 |  5600/28904 batches | lr 4.75 | ms/batch 1919.97 | loss  2.15 | ppl     8.56\n",
      "| epoch   2 |  5800/28904 batches | lr 4.75 | ms/batch 1930.80 | loss  2.12 | ppl     8.34\n",
      "| epoch   2 |  6000/28904 batches | lr 4.75 | ms/batch 1938.10 | loss  2.15 | ppl     8.56\n",
      "| epoch   2 |  6200/28904 batches | lr 4.75 | ms/batch 1934.85 | loss  2.14 | ppl     8.48\n",
      "| epoch   2 |  6400/28904 batches | lr 4.75 | ms/batch 1934.89 | loss  2.17 | ppl     8.73\n",
      "| epoch   2 |  6600/28904 batches | lr 4.75 | ms/batch 1919.32 | loss  2.23 | ppl     9.33\n",
      "| epoch   2 |  6800/28904 batches | lr 4.75 | ms/batch 1919.85 | loss  2.12 | ppl     8.37\n",
      "| epoch   2 |  7000/28904 batches | lr 4.75 | ms/batch 1931.13 | loss  2.21 | ppl     9.09\n",
      "| epoch   2 |  7200/28904 batches | lr 4.75 | ms/batch 1938.84 | loss  2.19 | ppl     8.96\n",
      "| epoch   2 |  7400/28904 batches | lr 4.75 | ms/batch 1933.75 | loss  2.32 | ppl    10.20\n",
      "| epoch   2 |  7600/28904 batches | lr 4.75 | ms/batch 1930.36 | loss  2.10 | ppl     8.19\n",
      "| epoch   2 |  7800/28904 batches | lr 4.75 | ms/batch 1934.61 | loss  2.16 | ppl     8.69\n",
      "| epoch   2 |  8000/28904 batches | lr 4.75 | ms/batch 1942.20 | loss  2.11 | ppl     8.26\n",
      "| epoch   2 |  8200/28904 batches | lr 4.75 | ms/batch 1935.45 | loss  2.19 | ppl     8.90\n",
      "| epoch   2 |  8400/28904 batches | lr 4.75 | ms/batch 1917.84 | loss  2.20 | ppl     9.07\n",
      "| epoch   2 |  8600/28904 batches | lr 4.75 | ms/batch 1916.05 | loss  2.22 | ppl     9.24\n",
      "| epoch   2 |  8800/28904 batches | lr 4.75 | ms/batch 1915.73 | loss  2.17 | ppl     8.77\n",
      "| epoch   2 |  9000/28904 batches | lr 4.75 | ms/batch 1919.98 | loss  2.18 | ppl     8.85\n",
      "| epoch   2 |  9200/28904 batches | lr 4.75 | ms/batch 1915.79 | loss  2.14 | ppl     8.53\n",
      "| epoch   2 |  9400/28904 batches | lr 4.75 | ms/batch 1916.03 | loss  2.18 | ppl     8.86\n",
      "| epoch   2 |  9600/28904 batches | lr 4.75 | ms/batch 1912.11 | loss  2.13 | ppl     8.42\n",
      "| epoch   2 |  9800/28904 batches | lr 4.75 | ms/batch 1918.82 | loss  2.15 | ppl     8.56\n",
      "| epoch   2 | 10000/28904 batches | lr 4.75 | ms/batch 1916.62 | loss  2.11 | ppl     8.25\n",
      "| epoch   2 | 10200/28904 batches | lr 4.75 | ms/batch 1916.44 | loss  2.06 | ppl     7.84\n",
      "| epoch   2 | 10400/28904 batches | lr 4.75 | ms/batch 1917.48 | loss  2.07 | ppl     7.90\n",
      "| epoch   2 | 10600/28904 batches | lr 4.75 | ms/batch 1914.76 | loss  2.10 | ppl     8.17\n",
      "| epoch   2 | 10800/28904 batches | lr 4.75 | ms/batch 1924.40 | loss  2.11 | ppl     8.21\n",
      "| epoch   2 | 11000/28904 batches | lr 4.75 | ms/batch 1936.58 | loss  2.21 | ppl     9.07\n",
      "| epoch   2 | 11200/28904 batches | lr 4.75 | ms/batch 1950.69 | loss  2.11 | ppl     8.28\n",
      "| epoch   2 | 11400/28904 batches | lr 4.75 | ms/batch 1934.37 | loss  2.14 | ppl     8.46\n",
      "| epoch   2 | 11600/28904 batches | lr 4.75 | ms/batch 1936.06 | loss  2.10 | ppl     8.20\n",
      "| epoch   2 | 11800/28904 batches | lr 4.75 | ms/batch 1921.05 | loss  2.13 | ppl     8.40\n",
      "| epoch   2 | 12000/28904 batches | lr 4.75 | ms/batch 1942.87 | loss  2.16 | ppl     8.63\n",
      "| epoch   2 | 12200/28904 batches | lr 4.75 | ms/batch 1939.43 | loss  2.11 | ppl     8.24\n",
      "| epoch   2 | 12400/28904 batches | lr 4.75 | ms/batch 1960.03 | loss  2.08 | ppl     8.00\n",
      "| epoch   2 | 12600/28904 batches | lr 4.75 | ms/batch 1962.76 | loss  2.09 | ppl     8.09\n",
      "| epoch   2 | 12800/28904 batches | lr 4.75 | ms/batch 1953.68 | loss  2.07 | ppl     7.90\n",
      "| epoch   2 | 13000/28904 batches | lr 4.75 | ms/batch 1958.61 | loss  2.09 | ppl     8.10\n",
      "| epoch   2 | 13200/28904 batches | lr 4.75 | ms/batch 1969.86 | loss  2.10 | ppl     8.18\n",
      "| epoch   2 | 13400/28904 batches | lr 4.75 | ms/batch 1949.60 | loss  2.19 | ppl     8.95\n",
      "| epoch   2 | 13600/28904 batches | lr 4.75 | ms/batch 1942.45 | loss  2.21 | ppl     9.10\n",
      "| epoch   2 | 13800/28904 batches | lr 4.75 | ms/batch 1941.07 | loss  2.20 | ppl     9.06\n",
      "| epoch   2 | 14000/28904 batches | lr 4.75 | ms/batch 1947.67 | loss  2.17 | ppl     8.80\n",
      "| epoch   2 | 14200/28904 batches | lr 4.75 | ms/batch 1946.51 | loss  2.20 | ppl     9.04\n",
      "| epoch   2 | 14400/28904 batches | lr 4.75 | ms/batch 2016.74 | loss  2.15 | ppl     8.60\n",
      "| epoch   2 | 14600/28904 batches | lr 4.75 | ms/batch 2000.64 | loss  2.08 | ppl     8.00\n",
      "| epoch   2 | 14800/28904 batches | lr 4.75 | ms/batch 1973.12 | loss  2.14 | ppl     8.48\n",
      "| epoch   2 | 15000/28904 batches | lr 4.75 | ms/batch 2007.22 | loss  2.21 | ppl     9.12\n",
      "| epoch   2 | 15200/28904 batches | lr 4.75 | ms/batch 2007.51 | loss  2.20 | ppl     8.98\n",
      "| epoch   2 | 15400/28904 batches | lr 4.75 | ms/batch 1996.58 | loss  2.16 | ppl     8.63\n",
      "| epoch   2 | 15600/28904 batches | lr 4.75 | ms/batch 1987.77 | loss  2.12 | ppl     8.37\n",
      "| epoch   2 | 15800/28904 batches | lr 4.75 | ms/batch 1991.52 | loss  2.14 | ppl     8.52\n",
      "| epoch   2 | 16000/28904 batches | lr 4.75 | ms/batch 1982.47 | loss  2.11 | ppl     8.28\n",
      "| epoch   2 | 16200/28904 batches | lr 4.75 | ms/batch 1975.02 | loss  2.14 | ppl     8.50\n",
      "| epoch   2 | 16400/28904 batches | lr 4.75 | ms/batch 1979.11 | loss  2.11 | ppl     8.23\n",
      "| epoch   2 | 16600/28904 batches | lr 4.75 | ms/batch 1977.07 | loss  2.09 | ppl     8.06\n",
      "| epoch   2 | 16800/28904 batches | lr 4.75 | ms/batch 1979.52 | loss  2.09 | ppl     8.12\n",
      "| epoch   2 | 17000/28904 batches | lr 4.75 | ms/batch 1949.00 | loss  2.10 | ppl     8.19\n",
      "| epoch   2 | 17200/28904 batches | lr 4.75 | ms/batch 1994.32 | loss  2.04 | ppl     7.66\n",
      "| epoch   2 | 17400/28904 batches | lr 4.75 | ms/batch 1986.84 | loss  2.09 | ppl     8.08\n",
      "| epoch   2 | 17600/28904 batches | lr 4.75 | ms/batch 1949.51 | loss  2.13 | ppl     8.42\n",
      "| epoch   2 | 17800/28904 batches | lr 4.75 | ms/batch 1948.47 | loss  2.11 | ppl     8.26\n",
      "| epoch   2 | 18000/28904 batches | lr 4.75 | ms/batch 1935.46 | loss  2.12 | ppl     8.35\n",
      "| epoch   2 | 18200/28904 batches | lr 4.75 | ms/batch 1943.18 | loss  2.07 | ppl     7.94\n",
      "| epoch   2 | 18400/28904 batches | lr 4.75 | ms/batch 1938.27 | loss  2.11 | ppl     8.23\n",
      "| epoch   2 | 18600/28904 batches | lr 4.75 | ms/batch 1938.95 | loss  2.11 | ppl     8.27\n",
      "| epoch   2 | 18800/28904 batches | lr 4.75 | ms/batch 1931.73 | loss  2.08 | ppl     8.03\n",
      "| epoch   2 | 19000/28904 batches | lr 4.75 | ms/batch 1927.35 | loss  2.10 | ppl     8.21\n",
      "| epoch   2 | 19200/28904 batches | lr 4.75 | ms/batch 1929.02 | loss  2.12 | ppl     8.32\n",
      "| epoch   2 | 19400/28904 batches | lr 4.75 | ms/batch 1928.58 | loss  2.09 | ppl     8.09\n",
      "| epoch   2 | 19600/28904 batches | lr 4.75 | ms/batch 1928.39 | loss  2.11 | ppl     8.26\n",
      "| epoch   2 | 19800/28904 batches | lr 4.75 | ms/batch 1933.28 | loss  2.05 | ppl     7.75\n",
      "| epoch   2 | 20000/28904 batches | lr 4.75 | ms/batch 1927.30 | loss  2.14 | ppl     8.51\n",
      "| epoch   2 | 20200/28904 batches | lr 4.75 | ms/batch 1927.73 | loss  2.17 | ppl     8.78\n",
      "| epoch   2 | 20400/28904 batches | lr 4.75 | ms/batch 1929.85 | loss  2.14 | ppl     8.54\n",
      "| epoch   2 | 20600/28904 batches | lr 4.75 | ms/batch 1927.56 | loss  2.15 | ppl     8.61\n",
      "| epoch   2 | 20800/28904 batches | lr 4.75 | ms/batch 1923.27 | loss  2.18 | ppl     8.85\n",
      "| epoch   2 | 21000/28904 batches | lr 4.75 | ms/batch 1922.29 | loss  2.12 | ppl     8.31\n",
      "| epoch   2 | 21200/28904 batches | lr 4.75 | ms/batch 1925.33 | loss  2.10 | ppl     8.19\n",
      "| epoch   2 | 21400/28904 batches | lr 4.75 | ms/batch 1927.08 | loss  2.09 | ppl     8.11\n",
      "| epoch   2 | 21600/28904 batches | lr 4.75 | ms/batch 1924.38 | loss  2.17 | ppl     8.79\n",
      "| epoch   2 | 21800/28904 batches | lr 4.75 | ms/batch 1925.15 | loss  2.14 | ppl     8.49\n",
      "| epoch   2 | 22000/28904 batches | lr 4.75 | ms/batch 1935.95 | loss  2.19 | ppl     8.92\n",
      "| epoch   2 | 22200/28904 batches | lr 4.75 | ms/batch 1958.69 | loss  2.24 | ppl     9.38\n",
      "| epoch   2 | 22400/28904 batches | lr 4.75 | ms/batch 1932.96 | loss  2.21 | ppl     9.11\n",
      "| epoch   2 | 22600/28904 batches | lr 4.75 | ms/batch 1973.71 | loss  2.12 | ppl     8.30\n",
      "| epoch   2 | 22800/28904 batches | lr 4.75 | ms/batch 1952.45 | loss  2.20 | ppl     8.99\n",
      "| epoch   2 | 23000/28904 batches | lr 4.75 | ms/batch 1934.57 | loss  2.15 | ppl     8.57\n",
      "| epoch   2 | 23200/28904 batches | lr 4.75 | ms/batch 1964.11 | loss  2.17 | ppl     8.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hunte\\coding projects\\ipynbs to get good text data\\fixed_create.ipynb Cell 9\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     train(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model, val_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     val_ppl \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mexp(val_loss)\n",
      "\u001b[1;32mc:\\Users\\hunte\\coding projects\\ipynbs to get good text data\\fixed_create.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     src_mask \u001b[39m=\u001b[39m src_mask[:seq_len, :seq_len]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m output \u001b[39m=\u001b[39m model(data, src_mask)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, ntokens), targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hunte/coding%20projects/ipynbs%20to%20get%20good%20text%20data/fixed_create.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "torch.save(model.state_dict(), 'example_fixed.pt')\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
