{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtgsdk import Card\n",
    "from mtgsdk import Set\n",
    "from mtgsdk import Type\n",
    "from mtgsdk import Supertype\n",
    "from mtgsdk import Subtype\n",
    "from mtgsdk import Changelog\n",
    "import os\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import math\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import spatial\n",
    "import sklearn \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import hdbscan\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import sys\n",
    "from random import sample\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = json.load(open('cards_object.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = []\n",
    "\n",
    "file1 = open(\"example.train.txt\", \"w\")\n",
    "file2 = open(\"example.test.txt\", \"w\")\n",
    "file3 = open(\"example.valid.txt\", \"w\")\n",
    "\n",
    "random_list = sample(range(len(all_data)),math.floor(float(len(all_data))*.8))\n",
    "        \n",
    "formatted_dict = {} \n",
    "\n",
    "amount = 0\n",
    "\n",
    "def _addtodoc_(doc,relevant_key,relevant_dict):\n",
    "    file = doc\n",
    "    header = f\" = {relevant_key} = \"\n",
    "    file.write(header + '\\n\\n')\n",
    "    pattern = r'([^\\w\\s])'\n",
    "    replacement = r' \\1 '\n",
    "    formatted_dict[header] = []\n",
    "    for each in relevant_dict[relevant_key]:\n",
    "        subheader = re.sub(pattern, replacement, each)\n",
    "        subheader = f\" = = {subheader} = = \"\n",
    "        file.write(subheader + '\\n\\n')\n",
    "        try:\n",
    "            text = relevant_dict[relevant_key][each]\n",
    "        except:\n",
    "            text = 'null'\n",
    "        if(type(text) is list):\n",
    "            delete_these = []\n",
    "            for count2,element in enumerate(text):\n",
    "                if(type(element) is dict):\n",
    "                    for key_2 in element:\n",
    "                        sub_sub_header = re.sub(pattern, replacement, key_2)\n",
    "                        sub_sub_header = f\" = = = {sub_sub_header} = = = \"\n",
    "                        file.write(sub_sub_header + '\\n\\n')\n",
    "                        try:\n",
    "                            #print(element[key_2])\n",
    "                            #print(re.sub(pattern, replacement, element[key_2]))\n",
    "                            file.write(re.sub(pattern, replacement, element[key_2].replace(f\"{relevant_key}\", \"self\")) + '\\n\\n')\n",
    "                        except:\n",
    "                            file.write(\"invalid language type \\n \\n\")\n",
    "                    delete_these.append(count2)\n",
    "            for count3,counted_value in enumerate(delete_these):\n",
    "                text.pop(counted_value - count3)\n",
    "            text = ' '.join(text)\n",
    "        try:\n",
    "            text = re.sub(pattern, replacement, text.replace(f\"{relevant_key}\", \"self\"))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            file.write(re.sub(pattern, replacement, text) + '\\n\\n')\n",
    "        except:\n",
    "            file.write(\"invalid language type \\n \\n\")\n",
    "\n",
    "check = True\n",
    "   \n",
    "for count,key in enumerate(all_data):\n",
    "    if(count in random_list):\n",
    "        _addtodoc_(file1,key,all_data)\n",
    "    else:\n",
    "        if check is True:\n",
    "            _addtodoc_(file2,key,all_data)\n",
    "            check = False\n",
    "        else:\n",
    "            _addtodoc_(file3,key,all_data)\n",
    "            check = True\n",
    "            \n",
    "    # list_sub_headers = []\n",
    "    # header = f\" = {key} = \"\n",
    "    # file.write(header + '\\n\\n')\n",
    "    # formatted_dict[header] = []\n",
    "    # if(count<1000):\n",
    "    #     for each in all_data[key]:\n",
    "    #         subheader = f\" = = {each} = = \"\n",
    "    #         file.write(subheader + '\\n\\n')\n",
    "    #         try:\n",
    "    #             text = all_data[key][each]\n",
    "    #         except:\n",
    "    #             text = 'null'\n",
    "    #         if(type(text) is list):\n",
    "    #             delete_these = []\n",
    "    #             for count2,element in enumerate(text):\n",
    "    #                 if(type(element) is dict):\n",
    "    #                     for key_2 in element:\n",
    "    #                         sub_sub_header = f\" = = = {key_2} = = = \"\n",
    "    #                         file.write(sub_sub_header + '\\n\\n')\n",
    "    #                         try:\n",
    "    #                             file.write(element[key_2] + '\\n\\n')\n",
    "    #                         except:\n",
    "    #                             file.write(\"invalid language type \\n \\n\")\n",
    "    #                     delete_these.append(count2)\n",
    "    #             for count3,counted_value in enumerate(delete_these):\n",
    "    #                 text.pop(counted_value - count3)\n",
    "    #             text = ' '.join(text)\n",
    "    #         try:\n",
    "    #             file.write(str(text) + '\\n\\n')\n",
    "    #         except:\n",
    "    #             file.write(\"invalid language type \\n \\n\")\n",
    "             \n",
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc816d8e426fe616fe65e3e533b3a5ac980db8a3cd9a2aa4e549e9c05f63913d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
