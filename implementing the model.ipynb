{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hunte\\coding projects\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertForMaskedLM, BertTokenizer\n",
    "\n",
    "#checkpoint = torch.load('C:\\\\Users\\\\hunte\\\\coding projects\\\\ipynbs to get good text data\\\\example.pt')\n",
    "model_path = 'C:/Users/hunte/coding projects/ipynbs to get good text data/example_fixed.pt'\n",
    "# model_dict = torch.load(model_path)\n",
    "# print(model_dict)\n",
    "# print((model_dict.keys()))\n",
    "# model = model_dict['model']\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "<class '__main__.TransformerModel'>\n",
      "pos_encoder.pe tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 8.4147e-01,  5.4030e-01,  7.9074e-01,  ...,  1.0000e+00,\n",
      "           1.0965e-04,  1.0000e+00]],\n",
      "\n",
      "        [[ 9.0930e-01, -4.1615e-01,  9.6811e-01,  ...,  1.0000e+00,\n",
      "           2.1930e-04,  1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.5625e-01, -2.9254e-01,  9.0551e-01,  ...,  8.2490e-01,\n",
      "           5.2090e-01,  8.5362e-01]],\n",
      "\n",
      "        [[ 2.7050e-01, -9.6272e-01,  2.1917e-01,  ...,  8.2483e-01,\n",
      "           5.2100e-01,  8.5356e-01]],\n",
      "\n",
      "        [[-6.6395e-01, -7.4778e-01, -6.3742e-01,  ...,  8.2476e-01,\n",
      "           5.2109e-01,  8.5350e-01]]], device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.0493,  0.0749,  0.0089,  ..., -0.1834, -0.0171, -0.0850],\n",
      "        [ 0.0855, -0.1409, -0.0879,  ..., -0.1463,  0.0855, -0.2376],\n",
      "        [-0.1764, -0.0833, -0.0917,  ..., -0.0004, -0.0275, -0.0959],\n",
      "        ...,\n",
      "        [-0.0175,  0.0349,  0.1385,  ...,  0.0631,  0.0624,  0.0203],\n",
      "        [-0.0019, -0.0277, -0.0168,  ...,  0.0430,  0.0069, -0.0247],\n",
      "        [ 0.0362, -0.0159,  0.0337,  ..., -0.0306,  0.0326,  0.0378]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias tensor([-8.0555e-02,  2.4455e-02, -1.1339e-01,  4.7280e-02, -1.1118e-01,\n",
      "         1.2039e-01, -7.3224e-02,  1.5624e-01, -5.2703e-02, -6.3707e-02,\n",
      "        -1.6229e-01, -1.1257e-01,  4.2934e-02,  1.3345e-01,  5.9465e-02,\n",
      "        -8.7277e-02,  1.0974e-01,  2.5868e-01, -1.3179e-01, -4.4720e-02,\n",
      "        -1.3488e-02, -1.7318e-01, -1.7244e-01,  1.9363e-01,  1.3084e-01,\n",
      "         3.9904e-02, -1.7334e-01, -2.2280e-02, -3.9679e-02, -2.1808e-01,\n",
      "         7.1966e-02,  1.9701e-01, -3.5308e-01, -1.4941e-01, -2.0097e-01,\n",
      "         1.0300e-01,  1.3735e-01, -2.1549e-01, -6.5867e-02, -4.4116e-02,\n",
      "        -2.1829e-01, -2.9857e-02,  2.0554e-01,  1.1307e-01, -1.2850e-01,\n",
      "        -3.5472e-02, -1.1790e-02,  2.2301e-01, -8.3218e-02,  2.9009e-02,\n",
      "        -3.9159e-02, -1.4058e-01, -2.3612e-01,  3.6510e-03, -1.6765e-01,\n",
      "        -7.7794e-02, -9.5856e-03, -3.3393e-01, -1.4098e-01,  2.9564e-02,\n",
      "        -5.5602e-02,  3.4487e-02,  1.1281e-01, -2.4195e-01, -1.9372e-01,\n",
      "         2.0728e-01, -4.2253e-02, -3.6541e-01, -1.0940e-02, -9.4983e-02,\n",
      "        -1.2898e-01,  2.4111e-01, -2.0629e-02,  4.8771e-01,  6.2351e-02,\n",
      "         5.3925e-02,  2.5833e-01,  7.0537e-02,  2.1359e-01, -8.6023e-02,\n",
      "         3.7632e-02, -4.1487e-02,  1.5157e-01,  1.3581e-01, -5.9059e-02,\n",
      "         9.3436e-02, -1.3557e-01, -2.2509e-01,  1.5067e-01,  3.3575e-01,\n",
      "         1.0332e-01, -2.1791e-01, -7.5304e-02,  1.2555e-02,  1.2290e-01,\n",
      "         6.1661e-02, -6.3456e-02,  2.4123e-01,  1.7147e-01,  1.5674e-01,\n",
      "        -2.5899e-01, -7.8961e-03,  2.4773e-01,  5.0944e-02,  4.4416e-02,\n",
      "        -1.1836e-01, -4.6490e-01, -8.7033e-02, -8.4272e-02, -2.2338e-01,\n",
      "         1.2514e-01,  7.5073e-02, -2.1921e-01,  4.0895e-01, -1.2065e-02,\n",
      "         6.2267e-02,  1.8597e-01,  2.5975e-01, -4.9557e-02, -2.0467e-02,\n",
      "        -4.2465e-01,  1.2963e-01, -1.0442e-01, -7.0002e-02, -1.8771e-01,\n",
      "         1.2803e-01, -1.0254e-01,  3.5292e-02,  1.8470e-02, -2.4869e-01,\n",
      "        -3.1510e-01, -3.9720e-01, -2.8803e-01,  1.3373e-01, -1.7695e-01,\n",
      "         3.7337e-01, -4.8932e-01, -7.4204e-02,  1.6635e-01,  5.8826e-02,\n",
      "         5.6204e-02, -3.9938e-01, -9.8919e-02, -4.2079e-02, -2.1060e-01,\n",
      "         1.1367e-01, -5.3920e-02, -1.2976e-03, -5.4449e-02,  2.9630e-01,\n",
      "        -4.6561e-02,  2.5024e-01, -1.6977e-01,  1.7050e-01,  3.6786e-02,\n",
      "        -6.0907e-02,  5.1235e-01, -5.7815e-02, -2.5918e-01, -1.5337e-01,\n",
      "         1.3929e-01, -1.2229e-01,  1.1918e-02, -1.1561e-01, -2.1808e-01,\n",
      "         3.2084e-01, -1.2617e-01, -5.6198e-02,  1.8952e-02,  3.4560e-01,\n",
      "         2.2900e-01,  1.1341e-01, -1.2067e-01, -1.0686e-01, -1.9762e-01,\n",
      "        -9.3982e-03, -1.2973e-01,  1.3542e-01, -1.9476e-01,  1.7158e-01,\n",
      "        -5.9235e-02, -2.4456e-01, -5.9583e-02, -3.2547e-01,  2.2259e-01,\n",
      "         1.0339e-02,  8.2332e-02,  1.2505e-01, -4.8124e-01,  1.2850e-01,\n",
      "         8.4211e-02, -1.6958e-01,  1.2486e-01, -4.5331e-02, -1.1896e-01,\n",
      "        -3.4909e-02,  2.4255e-01,  2.1073e-01,  1.1787e-01,  2.0371e-01,\n",
      "        -5.9074e-08, -5.6738e-08,  7.9499e-09,  6.3182e-09, -9.3280e-08,\n",
      "         2.1740e-09,  4.4819e-08,  7.3059e-09,  3.0379e-08, -7.7528e-08,\n",
      "        -2.5311e-09,  1.9968e-08, -5.0529e-08, -4.3604e-08,  3.5410e-08,\n",
      "        -2.0131e-08, -1.2883e-08,  8.0653e-08, -1.0696e-07,  1.5063e-08,\n",
      "        -7.4236e-08, -2.6380e-08,  4.8658e-09,  5.2178e-09, -4.3364e-09,\n",
      "         7.5137e-08, -6.6991e-08,  1.1539e-09,  2.9504e-08, -4.0052e-09,\n",
      "        -2.7824e-08,  8.7766e-08, -7.7264e-08, -6.7191e-08, -6.3279e-08,\n",
      "         4.3105e-08, -7.4349e-08, -6.0204e-08,  2.2408e-08,  9.6392e-09,\n",
      "        -5.5130e-08, -1.1887e-08,  5.3793e-08,  6.8023e-08, -8.5332e-08,\n",
      "        -6.0884e-09, -2.6280e-08,  8.2978e-08,  1.1283e-08, -5.5900e-08,\n",
      "        -2.0557e-08, -7.2885e-08, -5.9489e-08,  1.4259e-08, -7.2641e-08,\n",
      "        -1.1384e-08, -6.6619e-08, -3.8433e-08, -4.9322e-09, -2.5834e-08,\n",
      "        -3.7268e-08,  7.3123e-08, -3.4789e-08, -4.9785e-08,  5.5751e-08,\n",
      "         8.8731e-08, -4.5804e-08, -1.1386e-07, -6.8319e-08,  1.4778e-08,\n",
      "        -5.1212e-08,  1.0512e-08, -2.4174e-08,  1.9510e-07,  1.0934e-07,\n",
      "         5.0582e-08,  5.7372e-08,  1.2532e-07,  1.1811e-08, -1.7643e-08,\n",
      "        -3.8965e-08, -7.3296e-08,  3.1798e-08, -3.2115e-08,  1.1483e-08,\n",
      "         2.8333e-08, -3.5795e-08, -5.9145e-08,  8.7454e-08,  7.8465e-08,\n",
      "         6.7643e-08, -2.9073e-08, -9.7299e-08, -4.3236e-09,  1.0729e-08,\n",
      "        -2.4109e-08, -3.9811e-08,  1.1930e-07,  2.3504e-09,  3.5500e-08,\n",
      "        -6.1430e-09, -1.4458e-08, -3.7122e-08, -1.8717e-08,  1.4394e-08,\n",
      "         5.3775e-09,  4.5066e-08, -3.6617e-08, -5.5130e-08,  1.2146e-08,\n",
      "        -4.9268e-10, -1.4733e-08, -4.7424e-09, -5.1230e-08,  4.3623e-08,\n",
      "         2.4371e-08, -6.2385e-08,  5.1399e-08,  3.6773e-08, -3.6921e-08,\n",
      "         5.5557e-08, -6.5134e-09, -2.3827e-08, -3.2910e-08, -1.6565e-08,\n",
      "         4.5003e-08, -1.3169e-09, -3.4730e-09, -1.3222e-08,  4.1214e-10,\n",
      "         4.8819e-09, -6.8027e-08, -1.0950e-08,  3.0172e-08,  6.5541e-08,\n",
      "        -1.5973e-08, -1.2398e-08,  2.0185e-08, -4.5700e-08,  1.2432e-08,\n",
      "        -1.4116e-08,  3.6398e-08,  2.8538e-08, -3.2851e-08,  1.4839e-08,\n",
      "         6.0424e-11, -9.7429e-09,  2.3982e-08, -4.4386e-09, -2.5544e-08,\n",
      "         5.0720e-08,  4.8700e-08,  3.8918e-08, -1.8708e-08,  3.5032e-08,\n",
      "        -3.1044e-08, -6.5425e-08, -4.6100e-08, -4.7288e-08, -2.7273e-08,\n",
      "        -5.3379e-10,  3.0477e-08, -5.4261e-09,  7.6410e-09,  6.3486e-08,\n",
      "        -1.7028e-08,  4.9782e-09, -5.0047e-08, -2.8087e-08, -2.6393e-08,\n",
      "         5.0768e-08,  1.8433e-08,  3.6012e-09, -6.3557e-08,  2.8843e-08,\n",
      "        -2.7024e-08, -6.6084e-08, -5.0155e-08, -5.3335e-09,  2.3737e-08,\n",
      "        -2.5332e-09,  2.2003e-08,  4.2610e-10, -2.3719e-09, -3.5036e-08,\n",
      "        -1.8713e-08, -4.2745e-08, -4.0539e-09,  1.6047e-09,  3.2069e-08,\n",
      "        -1.5217e-08, -5.0025e-09, -2.9896e-08, -2.1452e-08, -2.9252e-09,\n",
      "        -1.3696e-08,  3.2146e-09,  5.1795e-10, -1.6252e-09,  8.5501e-09,\n",
      "        -1.4492e-02,  3.7719e-02,  2.8937e-02, -5.7541e-02,  1.3901e-01,\n",
      "         2.9972e-02,  2.2230e-02,  2.6678e-03, -1.3430e-01,  6.7153e-02,\n",
      "         6.0324e-03, -1.0968e-01,  3.2574e-02, -5.3024e-02,  6.2103e-02,\n",
      "        -1.3690e-02,  7.5418e-02,  3.8145e-02, -2.7814e-02, -1.4129e-01,\n",
      "         2.2346e-02, -3.9700e-02,  6.4688e-02,  1.5541e-02, -4.0591e-02,\n",
      "        -1.1024e-02,  1.5667e-01, -7.3783e-03, -3.7641e-02, -2.5937e-02,\n",
      "         4.9177e-02, -8.8156e-02,  2.5765e-02, -5.8892e-02,  4.8246e-02,\n",
      "         9.2130e-03, -4.6266e-02, -6.4172e-03,  2.4936e-03,  9.3423e-02,\n",
      "         3.9151e-03,  2.5287e-02, -2.7581e-02, -3.8511e-02, -5.6632e-02,\n",
      "         2.3858e-02,  4.0004e-02, -3.1287e-02,  4.7294e-03,  2.0278e-03,\n",
      "        -1.1947e-02,  2.3418e-02,  7.3579e-02,  4.0802e-02, -7.6368e-02,\n",
      "         6.5278e-02, -1.1717e-01,  4.1425e-02, -2.1113e-02, -4.8631e-02,\n",
      "        -1.8119e-03,  1.4254e-02,  6.3089e-02,  2.8037e-02,  4.7037e-02,\n",
      "        -1.2502e-02,  1.6491e-02,  4.3384e-02, -1.3929e-02, -3.1003e-02,\n",
      "         4.8590e-02,  3.0997e-02, -5.8259e-02,  2.6556e-02, -6.5472e-02,\n",
      "        -3.9019e-02, -2.7228e-02, -4.2967e-02,  3.5163e-02, -5.4967e-02,\n",
      "         2.4936e-02,  3.6253e-02, -1.2865e-02,  2.2907e-02,  1.0939e-02,\n",
      "         7.8056e-04,  5.0558e-02, -5.0347e-02,  9.0974e-02,  4.3524e-02,\n",
      "         4.2896e-02, -1.4200e-02,  8.8869e-03, -4.3068e-04, -6.2219e-02,\n",
      "        -4.4032e-02,  9.2015e-03, -5.9391e-02,  7.0122e-02,  7.3822e-03,\n",
      "         1.4941e-02,  4.1313e-02, -1.4635e-02, -4.2753e-02,  3.1633e-02,\n",
      "         4.0082e-03,  5.1345e-02, -2.5073e-02,  6.6746e-02, -4.2991e-03,\n",
      "        -6.5294e-02, -2.9475e-02,  5.1545e-02,  1.5188e-02, -7.0154e-02,\n",
      "         3.3733e-02, -2.6727e-02,  4.4038e-03,  3.0857e-02,  2.4266e-02,\n",
      "         5.2818e-02, -8.3335e-03,  1.3107e-02,  6.0305e-04, -1.9156e-02,\n",
      "         1.0789e-01, -8.7257e-02, -4.6324e-02, -4.4272e-03, -2.8157e-02,\n",
      "         1.2246e-02,  7.4498e-02, -1.0494e-02, -5.5049e-02,  2.5292e-02,\n",
      "         7.4941e-02,  8.1332e-02, -2.8777e-02, -6.7734e-02,  2.1930e-02,\n",
      "         1.1588e-01,  4.9299e-02, -1.6146e-03,  4.1248e-02,  8.5718e-02,\n",
      "        -1.3866e-01,  7.4777e-02,  1.5012e-02, -3.4838e-02, -3.7076e-02,\n",
      "        -8.6734e-02,  8.9858e-02, -1.8696e-02, -3.8388e-02,  9.1403e-02,\n",
      "         2.9050e-03,  4.0524e-02,  6.1875e-02, -5.6245e-02,  3.6607e-02,\n",
      "        -1.6517e-02, -1.0757e-02, -7.1763e-02,  2.1169e-02,  3.0286e-02,\n",
      "         4.4097e-03, -3.2801e-02, -4.8542e-02, -6.1314e-02,  4.5889e-02,\n",
      "         1.3283e-02, -4.9908e-02, -1.2514e-02, -2.9739e-02, -6.3711e-02,\n",
      "        -5.6517e-02, -4.6190e-02,  3.3370e-02, -2.6766e-02,  5.2145e-02,\n",
      "         2.0686e-02,  7.2714e-02,  8.5021e-02, -3.2947e-02, -1.9645e-02,\n",
      "        -6.0481e-03,  5.8665e-02, -5.3531e-02,  3.8092e-03, -8.0919e-02,\n",
      "         5.7906e-02, -4.3127e-02, -1.2421e-01, -2.7337e-02,  5.7497e-02,\n",
      "        -3.0565e-02,  1.4678e-02,  6.3362e-02, -7.3746e-03,  1.0810e-01],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[ 0.0246,  0.0179, -0.0251,  ..., -0.0069,  0.0123,  0.0476],\n",
      "        [ 0.0254,  0.0924,  0.0386,  ...,  0.0103, -0.0406, -0.0019],\n",
      "        [-0.0273, -0.0124, -0.1117,  ..., -0.0532, -0.0804,  0.0242],\n",
      "        ...,\n",
      "        [ 0.0080, -0.0631, -0.0214,  ...,  0.0350,  0.0177,  0.0049],\n",
      "        [ 0.0067,  0.0358,  0.0135,  ...,  0.0336,  0.1169, -0.0801],\n",
      "        [-0.0340,  0.0277, -0.0305,  ...,  0.0096,  0.0147, -0.0531]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias tensor([-0.0489, -0.5093, -0.2482, -0.3568, -0.4217, -0.1833,  0.1235,  0.1271,\n",
      "        -0.3610, -0.7127, -0.2638, -0.0420,  0.1642, -0.3558, -0.3292,  0.0995,\n",
      "        -0.1548, -0.3748,  0.0324, -0.4052, -0.2645, -0.1823, -0.3017, -0.1320,\n",
      "        -0.3318, -0.4149,  0.2374, -0.5256,  0.0501,  0.0675, -0.1439, -0.2206,\n",
      "         0.2405,  0.1928,  0.4483,  0.0138, -0.1573, -0.1097, -0.4387, -0.1680,\n",
      "        -0.0481,  0.0076,  0.2960, -0.2506,  0.3497, -0.2602,  0.2281,  0.2474,\n",
      "        -0.3583,  0.5316, -0.1327,  0.2631,  0.2961,  0.3021, -0.1027,  0.0796,\n",
      "        -0.2235,  0.1218, -0.1646,  0.5306, -0.0114,  0.3491, -0.2081,  0.5522,\n",
      "        -0.0386,  0.3347,  0.1186,  0.1403,  0.0460,  0.3455, -0.5243, -0.0225,\n",
      "        -0.3203,  1.1073, -0.4869,  0.3141, -0.0511,  0.3411, -0.1792,  0.2304,\n",
      "        -0.2645,  0.2320, -0.0150,  0.0845, -0.6877,  0.2293,  0.0739, -0.2633,\n",
      "         0.0545,  0.3858, -0.8021,  0.3667, -0.3734,  0.0326, -0.5708,  0.2610,\n",
      "         0.0022,  0.4033,  0.3374,  0.7889, -0.3093,  0.1816, -1.0063,  0.3476,\n",
      "        -0.2512,  0.7735, -0.4762,  0.4125, -0.1670,  0.4963, -0.0573, -0.0602,\n",
      "        -0.4526,  0.5112, -0.5582,  0.4761, -0.6360,  0.2046, -0.1604,  0.3796,\n",
      "         0.0475,  0.0694, -0.3496,  0.2295,  0.2297,  0.2032, -0.4945,  0.1579,\n",
      "         0.1222,  0.3570, -0.4252,  0.4419, -0.4826,  0.2512, -0.1952, -0.2138,\n",
      "        -0.0838,  0.4133, -0.4088,  0.1560, -0.3343,  0.5388, -0.6479,  0.0889,\n",
      "        -0.2402,  0.4736, -0.0594,  0.2721, -0.2695,  0.4590, -0.3472,  0.5545,\n",
      "         0.0607,  0.3435, -0.6775,  0.4368, -0.7234,  0.5812, -0.2267,  0.2714,\n",
      "        -0.0782,  0.2773, -0.2407,  0.1293, -0.2090,  0.6125, -0.0585, -0.0074,\n",
      "        -0.3025,  0.8299, -0.5748, -0.0288, -0.1288,  0.2580, -0.3486,  0.4885,\n",
      "        -0.3351,  0.7517, -0.1760,  0.3311, -0.4217,  0.4325, -0.7101,  0.2724,\n",
      "         0.0036,  0.6585, -0.4418,  0.2510, -0.2018,  0.1344, -0.3934,  0.6231,\n",
      "        -0.4096,  0.7135, -0.2727,  0.3711, -0.1140,  0.4880, -0.5412,  0.3097],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.weight tensor([[ 0.1331,  0.0170, -0.1110,  ...,  0.0367, -0.0158, -0.1338],\n",
      "        [ 0.0367,  0.0378, -0.0246,  ..., -0.0356,  0.2612,  0.0669],\n",
      "        [-0.1498,  0.0470,  0.2661,  ..., -0.0909,  0.0901, -0.0717],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0569, -0.1857,  ..., -0.0838,  0.2288,  0.0478],\n",
      "        [ 0.1238,  0.0498, -0.0359,  ...,  0.2262, -0.0474, -0.0146],\n",
      "        [ 0.1508, -0.0745, -0.1126,  ..., -0.1421, -0.1113,  0.0921]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.bias tensor([-0.1704, -0.3001, -0.3342, -0.4323, -0.3645, -0.3223, -0.4084, -0.2492,\n",
      "        -0.1714, -0.2825, -0.3804, -0.2585, -0.1822, -0.3060, -0.2810, -0.3051,\n",
      "        -0.2301, -0.1565, -0.1262, -0.2491, -0.2417, -0.2895, -0.2742, -0.3220,\n",
      "        -0.2986, -0.2250, -0.1394, -0.1182, -0.2611, -0.3563, -0.3566, -0.2968,\n",
      "        -0.2267, -0.3786, -0.2476, -0.2379, -0.2164, -0.1910, -0.6246, -0.2618,\n",
      "        -0.2843, -0.3758, -0.2268, -0.3733, -0.2775, -0.1776, -0.4139, -0.2894,\n",
      "        -0.3068, -0.5970, -0.2070, -0.2251, -0.2014, -0.3087, -0.2090, -0.2486,\n",
      "        -0.6744, -0.2555, -0.4384, -0.6802, -0.2927, -0.3020, -0.2410, -0.3445,\n",
      "        -0.3625, -0.3273, -0.1504, -0.1749, -0.3284, -0.4036, -0.2338, -0.2516,\n",
      "        -0.2903, -0.3273, -0.2542, -0.3585, -0.1441, -0.2036, -0.2473, -0.2363,\n",
      "        -0.2212, -0.3064, -0.2045, -0.4032, -0.2589, -0.3354, -0.3390, -0.1627,\n",
      "        -0.1933, -0.2558, -0.2410, -0.2855, -0.2380, -0.4877, -0.2291, -0.2086,\n",
      "        -0.2422, -0.2422, -0.3252, -0.1804, -0.2085, -0.2917, -0.3192, -0.5460,\n",
      "        -0.3302, -0.3095, -0.3214, -0.4069, -0.3957, -0.1837, -0.3369, -0.3833,\n",
      "        -0.2372, -0.2592, -0.2615, -0.3651, -0.1800, -0.4052, -0.3812, -0.2251,\n",
      "        -0.2716, -0.2715, -0.3491, -0.1848, -0.1571, -0.2770, -0.6182, -0.2649,\n",
      "        -0.2045, -0.3937, -0.4208, -0.2325, -0.1306, -0.2596, -0.2148, -0.2466,\n",
      "        -0.1707, -0.2577, -0.4495, -0.2462, -0.2881, -0.3235, -0.2450, -0.2201,\n",
      "        -0.2702, -0.2506, -0.1628, -0.2837, -0.4319, -0.2207, -0.4500, -0.4180,\n",
      "        -0.1156, -0.2317, -0.1373, -0.2972, -0.2873, -0.3084, -0.2595, -0.2545,\n",
      "        -0.6833, -0.2427, -0.1739, -0.3246, -0.3570, -0.4270, -0.2506, -0.2394,\n",
      "        -0.4075, -0.2164, -0.3129, -0.2551, -0.2903, -0.2451, -0.4449, -0.0928,\n",
      "        -0.3292, -0.2650, -0.3553, -0.1956, -0.3832, -0.7229, -0.1780, -0.1752,\n",
      "        -0.3621, -0.3599, -0.3685, -0.1628, -0.3709, -0.3112, -0.2900, -0.2726,\n",
      "        -0.1565, -0.3433, -0.2325, -0.3003, -0.5976, -0.2242, -0.2364, -0.1508],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.weight tensor([[ 0.0574,  0.0085,  0.0473,  ..., -0.1560,  0.1621,  0.0589],\n",
      "        [ 0.0697,  0.1005, -0.2027,  ..., -0.2176,  0.0818,  0.1096],\n",
      "        [-0.1338,  0.0071,  0.0715,  ...,  0.0166, -0.0150, -0.0430],\n",
      "        ...,\n",
      "        [-0.1517, -0.0060,  0.1010,  ...,  0.1686,  0.0367, -0.1086],\n",
      "        [ 0.0352,  0.1220, -0.0854,  ...,  0.0669, -0.2345, -0.1638],\n",
      "        [ 0.1156,  0.0980, -0.1361,  ..., -0.0136,  0.1843,  0.1451]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.bias tensor([ 0.2526,  0.2476, -0.0617,  0.0460,  0.4973,  0.1490, -0.1260,  0.1271,\n",
      "         0.3431,  0.3822,  0.0932,  0.1928,  0.2374,  0.2090, -0.1728,  0.0531,\n",
      "         0.2099,  0.0142, -0.0791,  0.3736,  0.1362,  0.2693,  0.1268,  0.0119,\n",
      "         0.0305,  0.2798, -0.0506, -0.1891,  0.1064, -0.0116, -0.1311,  0.1204,\n",
      "        -0.1271, -0.0635, -0.0890,  0.0806, -0.0464,  0.2823, -0.0992, -0.0986,\n",
      "        -0.0625, -0.0597,  0.1223,  0.0634,  0.0971,  0.5382,  0.0389,  0.0030,\n",
      "         0.1912,  0.0151,  0.0312,  0.0805, -0.2836,  0.1391, -0.2282,  0.1341,\n",
      "         0.1311,  0.0489, -0.0401, -0.1356, -0.1620, -0.0617, -0.0579, -0.1716,\n",
      "         0.1103,  0.0608,  0.0159, -0.0527,  0.2437,  0.1478, -0.0360, -0.0724,\n",
      "         0.0974, -0.5666, -0.1233,  0.0551, -0.0132, -0.0492,  0.0165, -0.0661,\n",
      "         0.0031, -0.0876, -0.0561, -0.0459, -0.0813,  0.0265,  0.1308,  0.0175,\n",
      "        -0.2069, -0.1032,  0.3467,  0.0660,  0.2560, -0.1028,  0.3872,  0.0585,\n",
      "        -0.1044, -0.1490,  0.0336,  0.0368,  0.1135, -0.1188,  0.3942, -0.0480,\n",
      "        -0.0664, -0.3234, -0.0017,  0.0279, -0.2155, -0.1031, -0.0447,  0.1088,\n",
      "         0.1879, -0.1760, -0.1693,  0.2368,  0.2835, -0.0173, -0.0866, -0.0534,\n",
      "        -0.0893,  0.0419,  0.2743,  0.2211,  0.0405,  0.2203,  0.0027, -0.3221,\n",
      "         0.0265,  0.0426,  0.3328, -0.0777, -0.2055,  0.0029, -0.0397,  0.0681,\n",
      "        -0.0595, -0.3056,  0.2824, -0.1795,  0.1459, -0.3016,  0.4220, -0.3918,\n",
      "         0.1705, -0.2208,  0.2671, -0.0708, -0.0426, -0.2087, -0.1395, -0.3691,\n",
      "        -0.1259, -0.0983,  0.2660, -0.0812,  0.1057, -0.5184, -0.0507, -0.2214,\n",
      "         0.0659,  0.1079, -0.0280,  0.1992,  0.2623, -0.2038,  0.3120,  0.1275,\n",
      "        -0.0802, -0.1238,  0.3530,  0.0141,  0.0545,  0.0167,  0.2528, -0.2571,\n",
      "        -0.0921, -0.1482,  0.1241, -0.0873, -0.1580, -0.1243,  0.1692, -0.4688,\n",
      "         0.0735, -0.4178, -0.1497, -0.1731,  0.0898,  0.0741,  0.2409, -0.2845,\n",
      "         0.0945, -0.2033,  0.1651,  0.0302,  0.0034,  0.0892,  0.2214, -0.0110],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.weight tensor([1.2545, 1.0189, 1.5862, 1.0705, 0.7140, 1.4129, 1.5121, 0.8913, 0.8645,\n",
      "        0.9574, 1.2506, 1.1782, 1.2973, 0.5870, 1.2472, 1.3571, 1.1055, 1.5554,\n",
      "        1.6674, 0.5903, 1.0573, 1.0595, 1.1846, 1.4867, 1.0372, 1.1084, 1.7336,\n",
      "        1.3388, 0.8290, 0.9817, 1.4691, 0.9303, 1.3127, 0.9869, 1.5477, 1.0700,\n",
      "        1.6271, 1.0184, 1.3538, 1.3670, 1.4039, 1.4682, 1.0242, 1.4934, 1.0797,\n",
      "        0.4175, 1.5364, 1.4150, 1.8840, 1.5694, 1.6346, 1.4289, 1.1310, 1.2455,\n",
      "        1.2896, 1.4283, 1.2306, 1.6638, 1.1121, 1.3438, 1.5749, 1.6366, 1.4782,\n",
      "        1.3204, 1.4280, 1.6320, 1.5721, 1.4559, 1.4484, 1.2725, 1.2579, 1.2409,\n",
      "        1.1985, 0.5033, 1.3522, 1.4413, 1.2526, 1.6058, 1.1516, 1.5489, 1.1525,\n",
      "        1.4457, 1.4035, 1.5686, 1.7002, 1.5254, 1.6670, 1.3682, 1.1916, 1.4900,\n",
      "        0.9800, 1.3664, 0.8661, 1.0819, 1.1369, 1.5311, 1.5879, 1.2306, 1.5884,\n",
      "        1.4927, 1.6159, 1.4725, 0.7395, 1.5649, 1.7658, 1.0724, 1.5447, 1.2165,\n",
      "        1.5850, 1.5065, 1.4087, 1.7786, 1.1219, 1.4856, 1.5440, 1.6024, 1.2582,\n",
      "        1.1806, 1.6984, 1.6282, 1.7376, 0.9267, 1.1576, 1.3533, 1.0699, 1.5066,\n",
      "        1.1545, 1.1858, 0.6590, 1.5263, 0.8610, 0.9999, 1.7458, 1.3492, 1.3696,\n",
      "        1.0830, 1.0815, 1.0046, 1.0040, 1.7521, 1.5685, 0.8600, 0.8378, 0.9396,\n",
      "        1.3969, 1.0455, 1.2010, 0.9732, 1.6299, 1.0324, 1.5897, 0.7278, 1.4389,\n",
      "        1.0468, 1.0305, 0.9790, 1.2861, 0.6506, 1.7870, 0.6825, 1.4249, 1.3400,\n",
      "        1.7123, 0.8660, 0.9371, 0.8759, 1.0037, 1.5563, 1.4762, 1.0281, 0.6960,\n",
      "        1.3934, 0.9518, 1.6858, 0.8004, 0.5677, 1.6629, 1.3928, 1.3354, 1.2207,\n",
      "        1.1733, 1.3971, 1.1209, 0.6512, 1.5050, 0.7550, 1.5744, 0.9145, 0.6566,\n",
      "        1.6028, 1.1760, 1.3655, 1.0412, 1.2054, 1.0059, 1.4723, 1.6810, 1.5621,\n",
      "        1.1638, 1.3913], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.bias tensor([-7.2806e-01, -3.9690e-01,  2.1244e-01, -5.2048e-01, -1.5749e+00,\n",
      "        -4.6768e-01,  5.3611e-03, -3.4846e-01, -1.3204e+00, -8.3233e-01,\n",
      "        -1.2778e-01, -1.4891e-01, -1.8611e-01, -1.0378e+00,  4.2757e-01,\n",
      "        -1.8286e-01, -3.7114e-01,  2.3828e-01,  2.5537e-01, -1.4118e+00,\n",
      "        -4.7040e-01, -6.7412e-01,  2.4331e-02,  7.6822e-03, -1.7426e-01,\n",
      "        -6.9120e-01, -1.5722e-01,  5.3644e-01, -7.4955e-01,  1.9940e-01,\n",
      "         2.5263e-01, -4.2191e-01,  1.2455e-01, -2.8828e-01, -2.7519e-01,\n",
      "        -4.8727e-01, -3.4236e-02, -8.5305e-01, -4.0795e-01, -8.9957e-02,\n",
      "        -1.8673e-01, -4.8652e-02,  6.7740e-02, -3.2053e-01, -4.4527e-01,\n",
      "        -2.0527e+00, -2.2967e-02,  1.3160e-01, -2.2894e-01,  1.6704e-01,\n",
      "         9.6867e-02, -1.7484e-01,  4.4804e-01, -4.4548e-01,  6.3457e-02,\n",
      "        -5.7867e-02, -3.2805e-01, -2.8425e-01,  4.2419e-04, -6.6460e-02,\n",
      "         2.0274e-01, -1.8672e-01,  3.4267e-01,  3.8025e-02, -3.0762e-01,\n",
      "        -2.3756e-01,  2.7418e-01,  5.1682e-01, -3.6461e-01, -2.0195e-01,\n",
      "         2.3861e-01,  3.2708e-01, -4.8317e-01,  2.2657e+00,  5.7408e-01,\n",
      "        -3.3045e-02, -9.6616e-02,  3.8623e-01,  7.3150e-02,  1.8046e-01,\n",
      "        -1.2681e-01,  7.2015e-01,  1.1038e-01,  2.0034e-01, -2.4250e-01,\n",
      "         1.1524e-03,  6.2499e-03,  5.2800e-01,  4.2377e-01,  5.6112e-01,\n",
      "        -1.2179e+00,  1.6344e-01, -9.2470e-01,  2.9721e-01, -7.0473e-01,\n",
      "        -6.2939e-03,  2.1234e-01,  5.9569e-01, -5.1099e-02,  3.7423e-01,\n",
      "        -2.1865e-01,  6.4797e-01, -1.0443e+00,  1.4246e-01, -3.1203e-01,\n",
      "         3.3568e-01,  4.6298e-02,  3.2907e-01,  1.3381e-01,  2.0687e-01,\n",
      "        -2.5746e-01,  3.5834e-02, -7.7009e-01,  3.3867e-01, -1.2497e-01,\n",
      "        -2.1428e-01, -5.2954e-01,  3.6667e-01, -1.1718e-02,  2.0858e-01,\n",
      "         9.2897e-02, -3.0223e-01, -9.7934e-01, -3.9751e-01, -5.0889e-02,\n",
      "         2.0258e-02, -1.1928e-01,  2.8826e-01, -1.6651e-01,  3.2150e-01,\n",
      "        -7.6795e-01,  7.3500e-01, -8.9398e-03,  3.4908e-01, -3.8321e-01,\n",
      "        -4.8421e-02,  3.5433e-01,  6.3684e-01, -1.0424e+00,  3.7742e-01,\n",
      "         6.4434e-02,  8.2449e-01, -1.1553e+00,  6.2204e-01, -3.5200e-01,\n",
      "         8.0789e-01, -6.5780e-01,  2.3952e-01, -6.1750e-03,  3.5634e-01,\n",
      "        -2.3459e-01,  9.2612e-01,  1.1794e-01, -2.2789e-01, -7.6112e-01,\n",
      "         6.6036e-01, -1.6403e-01,  1.5282e+00, -2.9126e-01,  6.9163e-01,\n",
      "        -4.9639e-01, -6.8398e-02, -7.1216e-02, -7.4424e-01, -9.7983e-01,\n",
      "         4.2144e-01, -6.3033e-01, -1.6118e-01,  2.6466e-02,  4.6641e-01,\n",
      "        -1.5562e+00,  5.8922e-02, -5.7227e-01,  2.7889e-01, -9.5339e-01,\n",
      "         1.1850e+00, -3.2165e-01,  4.6067e-01, -8.6096e-01,  6.1673e-01,\n",
      "         2.2845e-02,  4.8761e-01, -6.3248e-01,  1.2318e+00,  7.2054e-02,\n",
      "         1.0619e+00, -1.6823e-01,  5.4721e-02, -8.1097e-01,  4.5740e-02,\n",
      "        -2.5602e-01,  4.7102e-01, -1.7807e-01,  5.4072e-01, -6.3113e-01,\n",
      "         3.8453e-01, -1.7202e-01, -3.2429e-01, -9.6254e-01,  4.3207e-01],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.weight tensor([0.7405, 0.5238, 0.7584, 0.5306, 0.4973, 0.6910, 0.7299, 0.3340, 0.6398,\n",
      "        0.4295, 0.6272, 0.5512, 0.7211, 0.2686, 0.4524, 0.7866, 0.4957, 0.6516,\n",
      "        0.7063, 0.2661, 0.5526, 0.4396, 0.4678, 0.6649, 0.4736, 0.6375, 0.8455,\n",
      "        0.4820, 0.2777, 0.4404, 0.6225, 0.4677, 0.5946, 0.4802, 0.5921, 0.3673,\n",
      "        0.6928, 0.4149, 0.6149, 0.4659, 0.6875, 0.5610, 0.4492, 0.6957, 0.4893,\n",
      "        0.2947, 0.5904, 0.3785, 0.8771, 0.5077, 0.8505, 0.5230, 0.5652, 0.4401,\n",
      "        0.4775, 0.5776, 0.4241, 0.9042, 0.4300, 0.3896, 0.7466, 0.7088, 0.6190,\n",
      "        0.3585, 0.6680, 0.6812, 0.8286, 0.6795, 0.5563, 0.5424, 0.4562, 0.2787,\n",
      "        0.2896, 0.3482, 0.6782, 0.7925, 0.4180, 0.6323, 0.4685, 0.6942, 0.4582,\n",
      "        0.7659, 0.6093, 0.5940, 0.6738, 0.6592, 0.6702, 0.6462, 0.3831, 0.7076,\n",
      "        0.3908, 0.8426, 0.3106, 0.4971, 0.4064, 0.6523, 0.6556, 0.6060, 0.7976,\n",
      "        0.5811, 0.7867, 0.7206, 0.4046, 0.6607, 0.8438, 0.5370, 0.6021, 0.3688,\n",
      "        0.9080, 0.6461, 0.7534, 0.8242, 0.5417, 0.8092, 0.6401, 0.6366, 0.3939,\n",
      "        0.5326, 0.7284, 0.7061, 0.9186, 0.4611, 0.6108, 0.7793, 0.6065, 0.7183,\n",
      "        0.4660, 0.6306, 0.3555, 0.8889, 0.4515, 0.4344, 0.8100, 0.7880, 0.5637,\n",
      "        0.4704, 0.4957, 0.4664, 0.3643, 0.8385, 0.6603, 0.5213, 0.4081, 0.4700,\n",
      "        0.6313, 0.4477, 0.5683, 0.5002, 0.6765, 0.5368, 0.7811, 0.5255, 0.7666,\n",
      "        0.5461, 0.4471, 0.5720, 0.3245, 0.3385, 0.8152, 0.3210, 0.7083, 0.5312,\n",
      "        0.8096, 0.5088, 0.5166, 0.3994, 0.4113, 0.6596, 0.7013, 0.5237, 0.2602,\n",
      "        0.6526, 0.4037, 0.6885, 0.3197, 0.2786, 0.8534, 0.5578, 0.5421, 0.5238,\n",
      "        0.5694, 0.6136, 0.5085, 0.5158, 0.5423, 0.3651, 0.7109, 0.5002, 0.3213,\n",
      "        0.9233, 0.4302, 0.5440, 0.4206, 0.4348, 0.4828, 0.5961, 0.7780, 0.7381,\n",
      "        0.5768, 0.6684], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.bias tensor([ 0.1441,  0.0980, -0.1011,  0.0437,  0.2184,  0.0320, -0.0583, -0.0268,\n",
      "         0.2191,  0.1267,  0.0864,  0.0120, -0.0221, -0.1255, -0.0331, -0.0282,\n",
      "         0.0586, -0.1029, -0.0782, -0.0365,  0.0753,  0.1100, -0.0598,  0.0122,\n",
      "         0.0102,  0.0528, -0.0973, -0.1080,  0.0565, -0.1004, -0.0823, -0.0055,\n",
      "         0.0486,  0.0647,  0.0101,  0.0965, -0.0163,  0.1012, -0.0177,  0.0206,\n",
      "         0.0477, -0.0712, -0.0307,  0.0655, -0.0403,  0.1199,  0.0181, -0.0872,\n",
      "         0.0998, -0.1012,  0.0261, -0.0448, -0.0601,  0.0450, -0.0107, -0.1379,\n",
      "         0.1277,  0.0616,  0.0249, -0.0131, -0.0746, -0.0371, -0.0270, -0.0456,\n",
      "         0.0324,  0.0364, -0.1145, -0.2358,  0.1487,  0.0283, -0.0228,  0.0505,\n",
      "         0.0758, -0.1787, -0.1279, -0.1060,  0.0046, -0.0907, -0.0955, -0.0770,\n",
      "         0.0731, -0.1868, -0.0099, -0.0072,  0.1042, -0.0076,  0.0420, -0.0620,\n",
      "        -0.0634, -0.1178,  0.1677, -0.0648, -0.0497, -0.0522,  0.1086,  0.0282,\n",
      "         0.0146, -0.0615, -0.0500,  0.0037,  0.0927, -0.1405,  0.1532, -0.1026,\n",
      "         0.0480, -0.0554,  0.0014,  0.0532, -0.1218, -0.0287,  0.0898, -0.0292,\n",
      "         0.0927, -0.1741,  0.0345, -0.0050,  0.0801, -0.0108, -0.0259, -0.0130,\n",
      "         0.0495,  0.0322,  0.1948,  0.2354, -0.0632, -0.0908,  0.0375, -0.0231,\n",
      "         0.0552, -0.1261,  0.1161, -0.1101, -0.0198, -0.1117, -0.0672, -0.0142,\n",
      "        -0.0847, -0.1302,  0.1209, -0.1447,  0.0453, -0.1896,  0.2204, -0.0414,\n",
      "         0.1086, -0.0884,  0.0741,  0.0028,  0.0292, -0.0468,  0.0614, -0.1105,\n",
      "        -0.1236, -0.0141,  0.0604, -0.1060,  0.0472, -0.0956,  0.0025, -0.0805,\n",
      "         0.2033,  0.0544,  0.0504,  0.1364,  0.1158, -0.0314,  0.1205, -0.0164,\n",
      "        -0.0671, -0.0675, -0.0035, -0.0169,  0.0190, -0.0441,  0.0056, -0.0020,\n",
      "         0.1341, -0.0470,  0.2181, -0.1049,  0.0509, -0.1059,  0.0810, -0.2017,\n",
      "         0.0071, -0.0366, -0.1215,  0.0051, -0.0786, -0.0300,  0.1322, -0.1220,\n",
      "        -0.0311, -0.0064,  0.1325, -0.0662,  0.0198, -0.0241,  0.2711, -0.0721],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.self_attn.in_proj_weight tensor([[-0.0904,  0.0113,  0.0093,  ..., -0.0692, -0.0215, -0.0919],\n",
      "        [-0.0139, -0.1003, -0.0787,  ..., -0.0713,  0.1259, -0.0449],\n",
      "        [-0.1768, -0.0680, -0.1176,  ..., -0.0615,  0.0570, -0.0552],\n",
      "        ...,\n",
      "        [-0.0836, -0.0752,  0.0661,  ...,  0.0373, -0.0059,  0.0668],\n",
      "        [ 0.1522, -0.0984,  0.0436,  ..., -0.0255,  0.1606, -0.0045],\n",
      "        [-0.0179, -0.0418,  0.0335,  ..., -0.0711,  0.2515, -0.0574]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.self_attn.in_proj_bias tensor([-4.8216e-01, -1.9169e-01, -4.0465e-01,  6.8176e-02,  7.5014e-03,\n",
      "        -4.8991e-01, -1.3360e-01,  1.8917e-01,  1.3956e-01, -2.2300e-01,\n",
      "         1.6090e-01,  1.9404e-02, -1.6589e-01,  2.6582e-02, -1.8779e-01,\n",
      "         2.1177e-01,  9.0855e-02,  2.0449e-02,  8.1796e-02,  2.9211e-01,\n",
      "        -1.5565e-02,  2.2261e-02,  9.8205e-02,  8.1650e-02, -1.3488e-01,\n",
      "        -2.5710e-01, -4.8561e-01, -2.5642e-01, -3.2884e-01, -1.5855e-01,\n",
      "        -3.8874e-02,  2.0772e-01, -2.3605e-01,  3.0353e-01, -9.0584e-02,\n",
      "         3.3369e-01,  6.8880e-02, -2.5837e-02, -2.3398e-01,  1.4291e-01,\n",
      "        -2.5629e-02, -2.8820e-02,  5.4996e-02,  3.7509e-02,  8.9519e-02,\n",
      "        -2.4387e-01, -3.1156e-01, -1.8442e-02,  2.2621e-02, -2.1025e-01,\n",
      "         4.3771e-02, -8.6791e-02, -6.4690e-02,  1.7602e-01, -2.0565e-01,\n",
      "         1.5347e-01,  3.4827e-01,  9.3109e-02, -1.6973e-01, -2.1761e-02,\n",
      "        -6.2648e-02, -2.4036e-02,  5.7924e-02, -1.1544e-01,  8.2387e-02,\n",
      "         2.3942e-01, -2.4686e-01, -1.5743e-01, -1.6283e-01, -2.5914e-01,\n",
      "         4.4499e-01,  2.8854e-01, -3.9465e-01,  2.5969e-01,  1.7429e-01,\n",
      "         9.0345e-04,  1.5720e-01,  2.7491e-02, -3.1808e-01,  1.2613e-01,\n",
      "        -9.9851e-02,  5.2018e-01,  5.4783e-02,  1.4840e-01,  1.8667e-01,\n",
      "         8.6612e-02,  8.9981e-02, -2.9882e-01,  3.5629e-01,  2.1502e-01,\n",
      "         3.2615e-01, -2.8727e-01,  9.5223e-02, -4.6896e-02,  6.6655e-02,\n",
      "        -6.8233e-02, -1.3852e-01,  1.9153e-01, -2.5851e-01,  1.2084e-01,\n",
      "        -2.3543e-01,  2.7975e-01,  3.1188e-01,  2.0754e-01,  1.7525e-01,\n",
      "        -5.7178e-01, -3.7690e-01, -6.0548e-02, -1.9963e-01,  4.2726e-02,\n",
      "         3.8459e-01,  4.6389e-01,  3.8563e-01,  2.8055e-01, -7.0429e-01,\n",
      "        -4.3456e-02,  3.3728e-02,  2.4122e-01, -4.0261e-01,  1.2488e-01,\n",
      "        -4.3345e-01, -2.1182e-01,  1.5950e-01, -4.3325e-02, -9.3438e-02,\n",
      "         2.2244e-01,  1.0293e-01, -1.1032e-01, -2.9379e-01, -1.5211e-01,\n",
      "        -6.7918e-01,  2.9470e-02,  7.6429e-03, -1.9052e-01, -6.1399e-01,\n",
      "        -3.8138e-01, -3.2040e-01, -2.3593e-02,  3.9497e-01, -6.2455e-02,\n",
      "        -8.4234e-02, -3.5550e-01,  1.6914e-01, -5.1499e-02, -4.3740e-01,\n",
      "         8.2251e-02, -8.0357e-02, -2.0143e-01,  9.2268e-02, -8.0161e-03,\n",
      "        -4.5060e-01,  4.9112e-01,  2.9758e-01,  6.7985e-02,  1.0869e-01,\n",
      "        -9.5291e-03,  6.0777e-01,  3.0089e-01, -1.7889e-01, -2.8481e-01,\n",
      "        -2.3192e-01, -1.5296e-01,  3.9361e-01,  7.5485e-03,  1.4775e-01,\n",
      "        -1.1181e-02, -1.4840e-01,  4.4714e-01,  6.9615e-01,  4.4864e-01,\n",
      "         1.3104e-01, -1.8624e-01, -8.7013e-02, -1.8577e-02, -2.5189e-01,\n",
      "        -9.7412e-02,  3.3355e-02,  3.8767e-02, -2.9590e-01, -2.6133e-01,\n",
      "        -3.3496e-01,  2.3913e-01, -4.8196e-02, -1.4602e-01,  8.7165e-02,\n",
      "        -2.6948e-01,  7.7676e-02,  2.0292e-01,  1.9096e-01, -4.2958e-02,\n",
      "         2.1534e-01, -4.6539e-01, -1.7151e-01, -2.5259e-01, -2.1274e-01,\n",
      "         9.7977e-02, -8.0423e-02,  4.7383e-01,  4.0258e-02, -4.5954e-02,\n",
      "         2.4943e-08,  1.7201e-08, -6.1636e-09,  5.1898e-08,  2.5956e-10,\n",
      "         2.8638e-09, -2.1020e-08, -2.9148e-08,  5.2574e-08,  4.8396e-09,\n",
      "         5.5515e-08,  5.1597e-08, -7.1959e-09,  6.5199e-09,  3.9972e-09,\n",
      "         3.3139e-08,  2.7250e-08, -5.3409e-09, -2.4209e-08,  3.0361e-08,\n",
      "        -7.5779e-09,  3.6463e-08, -6.9343e-08, -3.4403e-08, -2.5322e-08,\n",
      "         1.9179e-08,  3.4475e-09,  1.2882e-08,  6.5998e-08,  2.0536e-08,\n",
      "        -4.2572e-08, -1.3448e-08,  2.5754e-08,  6.4412e-09,  2.7663e-08,\n",
      "         3.0323e-08,  4.8581e-08,  5.7024e-08,  7.6646e-09, -6.4196e-09,\n",
      "         3.1291e-09, -7.1988e-08,  3.0242e-08,  6.6177e-09, -6.2437e-08,\n",
      "        -8.2427e-09,  7.9488e-08, -8.1392e-09, -1.5273e-08,  1.3996e-08,\n",
      "        -1.2317e-08, -2.7248e-08,  1.5598e-08, -3.7886e-10,  4.4066e-08,\n",
      "         3.6161e-09, -2.7071e-08,  3.1708e-08,  9.6283e-08, -6.7450e-09,\n",
      "        -7.2280e-08,  1.4055e-09, -1.5255e-08,  4.7318e-08, -4.0593e-08,\n",
      "        -1.3881e-08,  4.8160e-08, -4.2183e-08, -1.4436e-08,  3.8864e-08,\n",
      "         1.0291e-08, -9.8728e-09, -4.6496e-09,  8.7778e-09, -1.7789e-08,\n",
      "         9.2781e-10, -2.4087e-08,  2.2474e-08, -4.0278e-08, -6.1979e-08,\n",
      "        -8.3046e-09, -3.0444e-08,  3.2569e-08, -8.9293e-09,  2.5282e-08,\n",
      "        -6.5298e-08, -4.6662e-08, -2.4344e-08, -7.7293e-08,  2.6965e-08,\n",
      "         4.1987e-08, -1.3102e-08,  8.8450e-09, -7.7521e-08,  2.9790e-08,\n",
      "        -4.7910e-08,  1.0224e-07,  4.9375e-08,  6.2907e-08, -1.4804e-08,\n",
      "        -1.7958e-08,  4.0469e-08, -3.1689e-10, -4.8273e-08,  2.0702e-08,\n",
      "         5.0436e-11,  3.1830e-08,  2.8657e-08,  3.8401e-08, -4.5251e-09,\n",
      "         2.2989e-09,  8.3609e-09,  2.5852e-08,  5.3078e-08, -1.4904e-08,\n",
      "         2.2023e-08,  2.5888e-08,  2.4119e-08, -3.6838e-09, -2.4309e-08,\n",
      "         1.1050e-07, -1.4843e-08,  6.9792e-09, -1.2189e-08,  1.1752e-09,\n",
      "        -1.0983e-09, -1.2390e-08, -4.3523e-08,  4.6154e-09,  5.2338e-08,\n",
      "        -1.9502e-08,  2.7875e-08, -3.6917e-08,  3.9852e-08, -2.7384e-08,\n",
      "         4.6123e-08,  5.6704e-08,  7.4981e-09,  3.2511e-10, -1.1846e-08,\n",
      "         9.0463e-08, -2.6351e-08,  7.4583e-09, -3.7729e-09, -3.2687e-08,\n",
      "         1.2592e-08, -5.0015e-08, -2.5102e-08,  2.0027e-08, -4.7344e-08,\n",
      "        -2.2637e-08, -1.5845e-08,  3.7428e-08,  2.9274e-08,  1.6306e-08,\n",
      "         2.4077e-08,  1.1432e-09,  3.7257e-08,  8.1115e-09,  2.0216e-08,\n",
      "         9.9306e-09, -3.9649e-08, -1.3758e-09,  7.7237e-09, -6.1071e-08,\n",
      "         5.7047e-08,  2.0954e-08, -6.4092e-09, -8.5915e-08, -5.0192e-08,\n",
      "         2.2768e-08, -9.2723e-09, -4.0107e-08, -3.4081e-08,  3.4228e-08,\n",
      "        -1.8405e-08,  4.5334e-09, -4.6429e-09, -2.4698e-08,  2.9967e-08,\n",
      "        -3.5517e-08, -2.5565e-08,  1.0076e-08, -6.1960e-08, -3.9895e-08,\n",
      "         1.8788e-08, -1.4549e-08, -2.0235e-08,  3.5309e-08, -7.1235e-09,\n",
      "         2.6715e-09,  2.4928e-08,  1.8233e-08, -4.5500e-08,  4.8819e-09,\n",
      "         1.3268e-08, -1.9142e-09, -1.8989e-09, -2.6456e-08, -8.2412e-09,\n",
      "        -5.8403e-02, -2.1967e-02,  3.5638e-02,  8.1301e-02,  5.5057e-02,\n",
      "         4.3431e-02,  3.6935e-02,  4.2188e-02,  7.6856e-04, -3.3154e-02,\n",
      "        -4.8227e-02, -2.7944e-02,  3.2013e-02, -1.8800e-02,  1.4411e-02,\n",
      "        -9.1450e-03, -2.0991e-02,  1.4544e-02, -3.2245e-02, -1.2445e-01,\n",
      "         3.4194e-02,  7.5589e-02, -5.3599e-02, -9.6615e-02, -2.2064e-02,\n",
      "         3.9507e-03, -1.6739e-02,  4.5471e-02,  1.6731e-02,  2.7448e-03,\n",
      "        -2.0093e-02,  2.5056e-02, -2.6058e-02, -1.6029e-02, -9.1496e-03,\n",
      "         6.0977e-02,  1.7759e-02,  1.1006e-02, -3.5113e-02,  4.9176e-02,\n",
      "        -8.1042e-03,  1.5493e-02,  7.5975e-03, -1.3535e-02,  1.3199e-02,\n",
      "        -2.8532e-02,  1.9357e-02, -2.1546e-02,  8.2428e-02,  6.3652e-02,\n",
      "        -1.0777e-01, -3.8835e-02, -3.5594e-02,  4.4409e-02,  5.8324e-02,\n",
      "         5.0785e-02, -7.5320e-02, -1.3166e-02, -8.7867e-02, -3.4926e-02,\n",
      "        -3.1958e-02,  2.8409e-02,  3.6584e-02, -1.7821e-02, -9.7139e-03,\n",
      "         7.5911e-02, -4.2205e-02,  1.4242e-02, -7.6849e-03, -2.9495e-02,\n",
      "         2.7563e-02,  2.2806e-02, -2.4211e-03,  4.7428e-02,  9.6757e-03,\n",
      "         4.1659e-02, -3.9317e-02,  3.8893e-03, -7.3069e-03, -7.4454e-02,\n",
      "         1.9071e-02,  5.6983e-02, -4.4491e-02,  1.1687e-02, -7.3335e-03,\n",
      "        -2.1453e-02, -4.4308e-02,  5.5301e-02, -3.3807e-02, -5.6706e-02,\n",
      "         7.6476e-02,  5.3494e-02, -1.6511e-02, -2.3042e-02,  1.3438e-02,\n",
      "        -8.5224e-03, -2.8449e-02, -4.4874e-02, -1.8106e-03,  1.8948e-02,\n",
      "        -1.9804e-02, -2.6865e-02, -2.3755e-03,  1.1991e-01, -7.1765e-03,\n",
      "        -3.5487e-02, -6.9410e-02, -1.7690e-02,  2.0694e-02, -7.7087e-04,\n",
      "         4.4711e-02,  1.5241e-02, -1.9972e-02,  9.3606e-03,  1.7237e-02,\n",
      "         6.9916e-04,  1.8345e-02,  4.3256e-02, -4.1587e-02, -2.4840e-02,\n",
      "         1.0039e-01, -3.2914e-02, -1.4029e-02,  3.0439e-02, -1.7961e-02,\n",
      "         7.3673e-02,  1.3199e-02,  1.1417e-02, -1.7492e-02,  5.8948e-02,\n",
      "         2.2125e-02, -3.7184e-02,  8.2905e-03,  7.2333e-02, -5.4135e-03,\n",
      "        -2.4352e-02, -4.6496e-02, -7.0890e-02,  3.7066e-02,  5.5516e-02,\n",
      "         1.6376e-02, -1.4497e-02,  1.6480e-02, -2.8529e-02,  2.5381e-02,\n",
      "        -4.4040e-02, -5.7596e-03, -3.9817e-02, -2.5207e-02, -3.0212e-03,\n",
      "         7.0855e-03, -4.9565e-02,  3.2259e-02, -4.3744e-02, -2.7664e-02,\n",
      "        -2.2262e-02, -3.0286e-02, -3.3307e-02, -1.4038e-02,  4.3456e-02,\n",
      "         8.4052e-02, -1.0798e-02, -3.9450e-03, -5.8033e-02, -3.1864e-02,\n",
      "        -1.0364e-02, -3.9658e-02, -3.6698e-02,  1.6953e-02,  9.3115e-03,\n",
      "         4.4102e-02, -6.8931e-03, -7.1065e-02,  2.3638e-02, -5.7809e-02,\n",
      "        -4.5567e-02, -3.6253e-02, -3.2662e-02,  3.6032e-02,  6.3309e-02,\n",
      "         4.4141e-02, -3.9377e-02,  2.9997e-02, -1.1547e-02, -2.9423e-02,\n",
      "        -4.2364e-02,  4.3621e-02, -3.1454e-02,  5.3782e-02, -3.2180e-02,\n",
      "        -1.0825e-01,  2.4012e-02,  2.2383e-02, -6.4607e-02,  3.9313e-02,\n",
      "        -4.3951e-02, -3.9468e-03,  3.5979e-02,  3.0521e-02,  3.3781e-02],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.self_attn.out_proj.weight tensor([[-0.0031,  0.0771,  0.0793,  ...,  0.1843,  0.0332,  0.0379],\n",
      "        [-0.0308, -0.1450, -0.1356,  ...,  0.0849, -0.0320,  0.0038],\n",
      "        [-0.0809, -0.0417,  0.0432,  ...,  0.0477,  0.0647,  0.0569],\n",
      "        ...,\n",
      "        [-0.1070,  0.0044, -0.0405,  ...,  0.1675, -0.0176,  0.0225],\n",
      "        [ 0.1576,  0.0316,  0.0025,  ...,  0.1102, -0.0776,  0.0135],\n",
      "        [ 0.0106, -0.1352,  0.0236,  ..., -0.0286, -0.0304, -0.0529]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.self_attn.out_proj.bias tensor([ 7.3543e-02,  1.2637e-02, -6.8841e-02, -1.7475e-01,  5.7925e-02,\n",
      "        -1.9995e-01, -1.1631e-01,  4.1248e-01, -5.6906e-02,  1.0696e-01,\n",
      "        -6.2442e-02, -2.4457e-01,  4.0373e-02,  2.6984e-02,  7.1462e-02,\n",
      "         7.8527e-02,  4.1658e-01, -1.1893e-01,  6.7028e-02, -1.6068e-01,\n",
      "        -6.2201e-02, -6.0431e-02, -7.1574e-02, -2.9491e-01,  2.5387e-01,\n",
      "        -1.4700e-01, -1.0986e-01, -2.4346e-01,  7.7173e-02,  1.9399e-01,\n",
      "         4.9282e-02,  2.0551e-01,  2.6985e-02,  1.3262e-01,  1.7982e-02,\n",
      "         3.1348e-01, -7.4675e-02,  5.5262e-02, -4.6417e-03, -2.4373e-01,\n",
      "         4.7193e-02,  1.5534e-01,  4.5470e-01, -1.3678e-01, -2.5122e-01,\n",
      "        -3.9535e-01,  1.1141e-01, -1.7938e-01, -2.4932e-02,  1.4353e-01,\n",
      "         1.3509e-01, -3.6330e-01,  4.0862e-01, -2.4917e-01, -1.0361e-01,\n",
      "        -2.1059e-01,  7.7679e-02, -8.8154e-02, -1.4686e-01,  1.6525e-01,\n",
      "        -9.3264e-02,  5.7532e-02, -5.4041e-02,  1.0855e-01, -5.6235e-03,\n",
      "         2.2340e-01, -7.5348e-03,  1.1412e-02,  9.1181e-02,  5.4767e-02,\n",
      "        -6.8679e-02,  1.8819e-02,  1.6946e-01,  9.4709e-02,  5.1230e-02,\n",
      "         1.6074e-01, -2.8214e-01,  1.3094e-01,  1.7674e-01, -1.2247e-01,\n",
      "        -2.2673e-01,  9.1475e-02, -3.8104e-02, -1.9410e-01, -3.1020e-02,\n",
      "         2.4936e-01,  8.0043e-02, -1.2085e-01,  6.3994e-02,  7.5164e-02,\n",
      "        -2.3289e-01,  6.1660e-02,  6.4671e-02,  1.8852e-02, -1.1153e-01,\n",
      "        -1.6520e-02,  8.2399e-02,  8.8800e-02, -7.3051e-02,  5.7793e-04,\n",
      "         3.0671e-02, -7.1760e-02, -1.5568e-01,  5.6487e-02, -1.6428e-01,\n",
      "        -1.7886e-01, -3.4903e-02,  3.0070e-01, -1.5201e-01,  1.9287e-01,\n",
      "        -1.5141e-01, -3.7643e-02,  1.0647e-02, -4.1003e-02, -6.9279e-02,\n",
      "         1.6465e-03, -1.2615e-01,  2.3472e-01, -2.9805e-02, -1.2559e-01,\n",
      "        -1.9434e-03, -4.3136e-02,  1.2741e-01, -1.0939e-01,  2.3292e-01,\n",
      "        -9.4832e-05,  1.2151e-01,  1.2779e-01,  5.0034e-01,  1.5189e-01,\n",
      "        -2.0273e-01,  1.1534e-01, -1.4942e-01,  2.4137e-02,  4.9197e-02,\n",
      "        -1.5310e-01,  5.1698e-02,  3.2295e-01, -1.9705e-01,  5.7721e-02,\n",
      "        -4.3334e-02,  7.0436e-02, -2.8595e-01, -2.9939e-01,  1.8860e-01,\n",
      "         1.5126e-01, -1.5382e-01,  1.5181e-01,  5.2970e-02, -8.0549e-02,\n",
      "        -4.6147e-02,  3.9335e-01, -2.6592e-02,  3.5148e-01,  1.1139e-01,\n",
      "         1.2290e-01,  3.8687e-02, -1.8340e-01, -1.0687e-01,  2.9148e-01,\n",
      "         4.3283e-02,  1.9108e-01, -2.4945e-02,  1.1913e-03,  3.8340e-01,\n",
      "         5.2923e-01,  1.7793e-01,  3.9584e-02, -1.1820e-01,  4.3780e-01,\n",
      "         4.9150e-02,  2.3920e-02,  1.0642e-01,  2.4765e-02, -6.9542e-02,\n",
      "         1.4356e-01,  5.5028e-02,  4.9855e-02,  4.7020e-02, -6.3797e-02,\n",
      "        -8.4411e-02, -1.3680e-01,  1.8151e-01,  2.5088e-01,  2.0513e-01,\n",
      "         1.3787e-01, -1.4846e-01,  5.0317e-01, -5.6631e-02, -7.0328e-02,\n",
      "         6.4722e-03,  1.5691e-01,  1.9190e-01,  5.0247e-01, -4.9356e-02,\n",
      "        -1.4881e-01, -5.5803e-02,  7.7103e-02,  8.4277e-02, -6.6790e-02],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.linear1.weight tensor([[-0.0913, -0.1744, -0.1073,  ..., -0.0775,  0.1173, -0.4072],\n",
      "        [-0.2424, -0.2987,  0.1511,  ...,  0.1111,  0.1539, -0.3786],\n",
      "        [-0.0263, -0.0197,  0.1005,  ...,  0.1051,  0.4322, -0.1348],\n",
      "        ...,\n",
      "        [-0.0266,  0.0712,  0.0784,  ...,  0.1835,  0.3775, -0.0995],\n",
      "        [ 0.2418, -0.0261,  0.0249,  ..., -0.0530,  0.1758,  0.0466],\n",
      "        [ 0.0535,  0.1294, -0.0960,  ...,  0.0860,  0.4439, -0.2999]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.linear1.bias tensor([-0.2257, -0.3943, -0.2227, -0.2380, -0.2885, -0.2916, -0.1021, -0.4180,\n",
      "        -0.2946, -0.5424, -0.0771, -0.2089, -0.2612, -0.4511, -0.2139, -0.4415,\n",
      "        -0.2410, -0.1533, -0.3409, -0.1796, -0.1378, -0.2710, -0.5603, -0.4093,\n",
      "        -0.2055, -0.2340, -0.3438, -0.3406, -0.1917, -0.2098, -0.3848, -0.0177,\n",
      "        -0.1816, -0.0747, -0.0949, -0.2079, -0.1622, -0.3063, -0.1602, -0.1352,\n",
      "        -0.3609, -0.5095, -0.1699, -0.1318, -0.3581, -0.2646, -0.1702, -0.4618,\n",
      "        -0.0895, -0.4041, -0.2222, -0.3933, -0.0600, -0.4335, -0.2916, -0.0866,\n",
      "        -0.3353, -0.1496, -0.3600, -0.3831, -0.4473, -0.0717, -0.4051, -0.5392,\n",
      "        -0.3423, -0.3289, -0.3107, -0.2247, -0.1496, -0.3262, -0.2573, -0.2475,\n",
      "        -0.2619, -0.3386, -0.2415, -0.2616, -0.3404, -0.1290, -0.4511, -0.2382,\n",
      "        -0.1557, -0.2539, -0.0622, -0.2595, -0.0727, -0.5931, -0.2227, -0.2199,\n",
      "        -0.1603, -0.2018, -0.4160, -0.3642, -0.1022, -0.2541, -0.0892, -0.2224,\n",
      "        -0.2291, -0.1087, -0.1505, -0.1738, -0.0279, -0.1006, -0.2483, -0.2564,\n",
      "        -0.2367, -0.2414, -0.3891, -0.1845, -0.1132, -0.0867, -0.3409, -0.2481,\n",
      "        -0.2077, -0.1521, -0.1811, -0.3275, -0.0933, -0.4593, -0.4133, -0.0212,\n",
      "        -0.4649, -0.3471, -0.3255,  0.0199, -0.3910, -0.3196, -0.4270, -0.3253,\n",
      "        -0.1364, -0.4123, -0.1283, -0.4704, -0.2477, -0.1933, -0.2668, -0.2639,\n",
      "        -0.0789, -0.2467, -0.3762, -0.2372, -0.1804, -0.1348, -0.0759, -0.1495,\n",
      "        -0.1768, -0.1835, -0.0298, -0.3070, -0.2365, -0.1844, -0.2834, -0.2852,\n",
      "        -0.2040, -0.2244, -0.2841, -0.2211, -0.1448, -0.2108, -0.2448, -0.1218,\n",
      "        -0.2176, -0.3072, -0.1823, -0.1245, -0.4550, -0.3037, -0.0948, -0.3745,\n",
      "        -0.1330, -0.2177, -0.2823, -0.2427, -0.1871, -0.0837, -0.2460, -0.3009,\n",
      "        -0.1949, -0.1842, -0.3315, -0.3704, -0.1846, -0.5033, -0.1598, -0.0924,\n",
      "        -0.3766, -0.4131, -0.3428, -0.2072, -0.2729, -0.2322, -0.2033, -0.0896,\n",
      "        -0.2622, -0.2737, -0.2085, -0.0884, -0.2894, -0.2535, -0.0228, -0.4885],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.linear2.weight tensor([[-0.0244, -0.0574,  0.0195,  ...,  0.0550,  0.0465,  0.2238],\n",
      "        [-0.0217,  0.0155, -0.1061,  ...,  0.0836,  0.0202, -0.4123],\n",
      "        [ 0.0635,  0.1096, -0.0574,  ...,  0.0043, -0.0127,  0.0026],\n",
      "        ...,\n",
      "        [-0.0843,  0.1460,  0.0168,  ..., -0.0993,  0.0168,  0.2262],\n",
      "        [-0.1088, -0.3978, -0.0569,  ..., -0.0528,  0.0346, -0.0969],\n",
      "        [-0.0479,  0.1570,  0.0968,  ...,  0.0354,  0.1297,  0.1821]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.linear2.bias tensor([ 0.1213, -0.0683,  0.0188, -0.0681,  0.3163,  0.0741,  0.1014,  0.1988,\n",
      "         0.5150, -0.2396,  0.1239, -0.1317,  0.1783,  0.2747,  0.0031, -0.2465,\n",
      "         0.3434,  0.0487, -0.1200,  0.4190,  0.1215,  0.2201, -0.2478, -0.2380,\n",
      "         0.1580,  0.3590,  0.0160, -0.1845,  0.6013,  0.5560, -0.1898,  0.6212,\n",
      "         0.1041, -0.1500, -0.0211, -0.0869, -0.0827,  0.2084,  0.0060, -0.1995,\n",
      "         0.1214,  0.0918,  0.0903,  0.3655,  0.1745,  0.2731, -0.1044, -0.0873,\n",
      "         0.0859, -0.0835, -0.0746,  0.1121,  0.5047,  0.1735, -0.3149,  0.0557,\n",
      "         0.2117,  0.0142,  0.3644,  0.0288, -0.0291,  0.0063, -0.1506,  0.1808,\n",
      "         0.0381,  0.0470, -0.0257, -0.0353,  0.0539,  0.0257, -0.1223,  0.2208,\n",
      "         0.1484,  0.5398, -0.0801,  0.1343, -0.2200, -0.0085,  0.1152, -0.0746,\n",
      "        -0.1374, -0.0996, -0.1262,  0.1376, -0.0616, -0.0923, -0.1996, -0.0043,\n",
      "        -0.2859, -0.1407,  0.4746, -0.0735,  0.1727,  0.1307,  0.2837,  0.3696,\n",
      "        -0.2297,  0.0659,  0.0514, -0.0353,  0.0897, -0.2055,  0.4380,  0.0826,\n",
      "         0.0178, -0.4276, -0.0704, -0.0168,  0.0717, -0.1677, -0.1853,  0.1007,\n",
      "         0.3247, -0.2213,  0.0342,  0.1731,  0.2493, -0.0595,  0.0933, -0.0907,\n",
      "        -0.1401,  0.0041,  0.5580,  0.0376,  0.0806,  0.0645, -0.3197, -0.0135,\n",
      "         0.7917, -0.0434,  0.1444, -0.0698, -0.0756,  0.0620,  0.0887,  0.0312,\n",
      "         0.7904, -0.0413,  0.0581, -0.0731, -0.0632,  0.0186,  0.4830, -0.0691,\n",
      "         0.0323, -0.1948,  0.1259, -0.0158, -0.0405, -0.2652,  0.0770,  0.2874,\n",
      "        -0.0062, -0.2615,  0.2703, -0.1364,  0.0597, -0.0329, -0.0391,  0.0061,\n",
      "         0.0813, -0.0173, -0.1399, -0.0244,  0.1824,  0.2123,  0.2198,  0.0445,\n",
      "        -0.0550,  0.7722,  0.4281,  0.1324, -0.0191,  0.0504,  0.1211, -0.3004,\n",
      "        -0.0138, -0.3173,  0.1894, -0.2226,  0.4936, -0.2937,  0.1201, -0.2079,\n",
      "        -0.0245, -0.2764,  0.0189,  0.4909,  0.1339, -0.0870, -0.1010, -0.2404,\n",
      "         0.8533, -0.1574,  0.0043, -0.1183, -0.0291,  0.0710,  0.3419, -0.1544],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.norm1.weight tensor([1.2364, 1.0336, 1.5544, 1.1583, 0.9430, 1.5840, 1.6791, 1.5430, 0.8153,\n",
      "        0.8905, 1.1584, 1.2046, 1.4418, 1.4061, 1.1434, 1.5226, 0.9303, 1.6198,\n",
      "        1.8272, 1.3651, 1.1093, 0.8875, 1.0401, 1.3458, 0.9280, 1.1609, 1.7408,\n",
      "        1.1540, 1.1899, 1.0386, 1.0962, 1.0579, 1.1731, 0.8845, 1.4702, 0.9559,\n",
      "        1.4585, 0.9585, 1.5041, 0.9372, 1.3549, 1.3932, 1.1137, 1.4614, 0.8923,\n",
      "        0.8371, 1.4868, 1.0141, 1.7354, 1.2200, 1.7372, 1.0430, 1.0608, 0.9723,\n",
      "        0.6075, 1.4853, 1.1434, 1.5441, 0.6889, 1.0677, 1.5697, 1.6301, 1.5144,\n",
      "        0.9598, 1.2886, 1.2768, 1.6930, 1.5990, 1.3738, 1.0767, 1.0067, 1.0761,\n",
      "        1.5326, 0.9246, 1.6393, 1.4493, 0.8798, 1.5229, 0.8960, 1.4739, 1.0234,\n",
      "        1.5108, 1.6589, 1.4162, 1.4255, 1.3628, 1.7108, 1.5641, 0.9362, 1.6345,\n",
      "        0.5181, 1.4781, 1.5718, 1.2237, 0.8051, 1.1420, 1.8676, 1.4545, 1.5850,\n",
      "        1.5466, 1.7114, 1.3931, 0.9024, 1.6303, 1.7732, 0.9085, 1.4319, 1.3080,\n",
      "        1.7874, 1.6945, 1.4525, 1.7367, 1.0654, 1.4999, 1.4263, 1.5866, 0.8907,\n",
      "        1.2510, 1.7018, 1.4825, 1.7123, 1.1878, 1.2621, 1.3855, 1.1129, 1.7169,\n",
      "        0.9274, 1.1165, 1.0326, 1.7999, 0.6868, 1.3989, 1.8594, 1.5581, 1.4451,\n",
      "        1.0364, 1.1544, 1.0686, 1.4699, 2.0118, 1.3923, 0.9719, 1.0111, 0.8005,\n",
      "        1.2160, 1.3138, 0.8872, 1.2226, 1.7856, 1.1537, 1.5213, 0.8059, 1.5216,\n",
      "        0.8114, 1.1283, 1.5494, 1.2204, 1.1586, 1.6850, 1.3147, 1.5282, 1.3020,\n",
      "        1.5854, 1.0806, 1.0962, 0.9640, 1.0044, 1.5880, 1.6652, 1.1575, 1.4576,\n",
      "        1.4460, 0.9653, 1.5586, 1.5137, 1.3425, 1.5639, 1.2764, 1.0627, 1.3489,\n",
      "        1.0423, 1.3522, 1.0641, 1.1332, 1.2825, 1.1500, 1.5836, 1.2961, 1.6443,\n",
      "        1.6065, 1.1781, 1.3666, 1.1413, 1.0623, 0.8427, 1.3433, 1.5714, 1.5829,\n",
      "        0.8322, 1.1668], device='cuda:0')\n",
      "transformer_encoder.layers.1.norm1.bias tensor([ 0.1368,  0.0963, -0.3252, -0.0544, -1.5653, -0.2057, -0.1480, -0.3426,\n",
      "        -1.5289,  0.5166, -0.1140,  0.4049, -0.6991, -0.7345, -1.0088,  0.1387,\n",
      "         0.3695,  0.0298, -0.3775, -0.7845, -0.3007, -0.5498,  1.3995,  0.4527,\n",
      "        -0.5548, -0.1851,  0.0199,  0.4927, -0.0774,  0.5621,  1.2247,  0.0071,\n",
      "        -0.2298,  1.0209, -0.4791, -0.0619,  0.0030, -0.7808,  0.0153,  1.6929,\n",
      "        -0.0781, -0.1735,  0.0343, -0.1787, -1.0132, -1.5946,  0.7511,  0.3894,\n",
      "         0.2039,  0.3022, -0.0394,  0.1451,  0.7347, -0.5378,  1.5424,  0.7315,\n",
      "        -0.1285, -0.0258, -1.0421,  0.4083, -0.2269, -0.3081,  0.3046, -0.2875,\n",
      "        -0.9153, -0.1085, -0.2168,  0.5698, -0.1483,  0.1362,  1.3754,  0.0525,\n",
      "        -1.0991,  0.2106,  0.9469, -0.2437,  0.4623, -0.1405,  0.8512,  0.0920,\n",
      "         0.3695,  0.6976, -0.4866, -0.4442,  0.0941, -0.2897,  0.3360,  0.3392,\n",
      "         0.6368, -0.0410, -2.2726,  0.3183, -0.4207, -0.6845, -0.7629, -0.4529,\n",
      "         0.3585, -0.2578,  0.3558,  0.1478,  0.3548,  0.4207, -1.5488, -0.2059,\n",
      "         0.2573,  1.3904,  0.5038,  0.3572,  0.1837, -0.1576,  0.3247,  0.1337,\n",
      "        -0.6665,  0.3860, -0.1188, -0.5371, -0.8294, -0.1686, -0.1429,  0.7309,\n",
      "         0.2921, -0.1457, -1.2450,  0.1714,  0.0073, -0.2859,  0.8515, -0.2037,\n",
      "         0.1489,  0.1303, -1.3950, -0.1668,  0.0065,  0.0096, -0.2034, -0.0693,\n",
      "         0.0537, -0.6266, -0.1993,  0.1455,  0.4501,  0.9490, -0.3837,  1.1483,\n",
      "        -0.8261,  0.3923, -0.4995, -0.2149, -0.4317,  0.2742, -0.5766,  0.5667,\n",
      "        -0.3000,  0.7314, -0.0428, -0.7094,  0.0484,  0.6969,  0.1869, -0.0393,\n",
      "         0.0246, -0.2384,  0.3777, -0.0850,  0.1044,  0.6185,  0.1482,  0.0533,\n",
      "         0.2355,  0.6848, -0.2396,  0.2265,  0.1755, -0.1379, -0.5526,  0.5796,\n",
      "        -0.1161,  0.5390, -1.3034,  0.5537, -0.4336,  0.3241, -0.2422,  0.4171,\n",
      "        -0.2886, -0.3253, -0.3359,  0.2039, -0.3685,  0.3014, -0.2451,  0.1714,\n",
      "         0.1706,  0.1139, -0.1796,  0.7164, -0.0405, -0.5132, -2.2237,  1.4500],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.1.norm2.weight tensor([0.8414, 0.5633, 0.8448, 0.3939, 0.7417, 0.9521, 0.9069, 0.2362, 0.7497,\n",
      "        0.3560, 0.7708, 0.6057, 0.6946, 0.3116, 0.5861, 0.6172, 0.1404, 0.7545,\n",
      "        0.9073, 0.3587, 0.4294, 0.4157, 0.5488, 0.5911, 0.4779, 0.7186, 1.0157,\n",
      "        0.6637, 0.0968, 0.1281, 0.7097, 0.0976, 0.5093, 0.4085, 0.7045, 0.2163,\n",
      "        0.9317, 0.4417, 0.7922, 0.6303, 0.6532, 0.7423, 0.1435, 0.6953, 0.4375,\n",
      "        0.3652, 0.6911, 0.4627, 0.9704, 0.7791, 0.9198, 0.5354, 0.2830, 0.5015,\n",
      "        0.5210, 0.6765, 0.3888, 1.0153, 0.3417, 0.5084, 0.8424, 0.9869, 0.7630,\n",
      "        0.3650, 0.8483, 0.8964, 0.9467, 0.7115, 0.7533, 0.4442, 0.5758, 0.4870,\n",
      "        0.3840, 0.0989, 0.7530, 0.9755, 0.2219, 0.9582, 0.2579, 0.8193, 0.5523,\n",
      "        0.8777, 0.5642, 0.6516, 0.7831, 0.6216, 0.7228, 0.6139, 0.3557, 0.6803,\n",
      "        0.3597, 0.8030, 0.2695, 0.4694, 0.4081, 0.7732, 0.7494, 0.7180, 0.7110,\n",
      "        0.5283, 0.7793, 0.7711, 0.2556, 0.8623, 0.9652, 0.5578, 0.6696, 0.5377,\n",
      "        0.9404, 0.6642, 0.8884, 0.9666, 0.6420, 0.8098, 0.5837, 0.6899, 0.4862,\n",
      "        0.5486, 0.8493, 0.9136, 0.9163, 0.2127, 0.5197, 0.9053, 0.2374, 0.7953,\n",
      "        0.2877, 0.5123, 0.1137, 0.7396, 0.4505, 0.4663, 0.9197, 0.8241, 0.4498,\n",
      "        0.3568, 0.1715, 0.5839, 0.2980, 0.9045, 0.8248, 0.4070, 0.3636, 0.6251,\n",
      "        0.7688, 0.4769, 0.5060, 0.3870, 0.7342, 0.5418, 0.8767, 0.1474, 0.7158,\n",
      "        0.1705, 0.1545, 0.5102, 0.4968, 0.6796, 1.0058, 0.4720, 0.5705, 0.4734,\n",
      "        0.9340, 0.5179, 0.1866, 0.2193, 0.1154, 0.8965, 0.7736, 0.1799, 0.2280,\n",
      "        0.7596, 0.3186, 0.9459, 0.2334, 0.4799, 0.8733, 0.6340, 0.4970, 0.4843,\n",
      "        0.0469, 0.6298, 0.3976, 0.1440, 0.7127, 0.5769, 0.8846, 0.2695, 0.2951,\n",
      "        0.9563, 0.4358, 0.6067, 0.0315, 0.1689, 0.2930, 0.6819, 0.9479, 0.7962,\n",
      "        0.5171, 0.8161], device='cuda:0')\n",
      "transformer_encoder.layers.1.norm2.bias tensor([-0.1122,  0.1870,  0.2371,  0.0444,  0.2710,  0.2171,  0.2048, -0.1757,\n",
      "         0.1411,  0.0107,  0.3771,  0.2357,  0.0856,  0.0865,  0.2117,  0.1354,\n",
      "        -0.1055,  0.0262,  0.2209, -0.2238,  0.0992, -0.0320, -0.1786,  0.2076,\n",
      "         0.0404, -0.0080,  0.0525,  0.0952, -0.0185, -0.2265, -0.0916, -0.1358,\n",
      "         0.1734, -0.1959,  0.2006, -0.2410,  0.1856,  0.0526,  0.0562, -0.1903,\n",
      "         0.0320, -0.1602, -0.0882, -0.0511,  0.1524,  0.1789,  0.3576,  0.2214,\n",
      "        -0.0147,  0.1061,  0.1415, -0.2481, -0.4272,  0.4135, -0.2141, -0.0141,\n",
      "         0.0137,  0.0430,  0.1809, -0.0892,  0.2188,  0.1025,  0.1045, -0.0219,\n",
      "         0.4172, -0.0686,  0.1833, -0.0536,  0.1632,  0.0009, -0.0503,  0.3007,\n",
      "        -0.1611, -0.0806, -0.1057, -0.1635, -0.0294,  0.1071, -0.1318,  0.0456,\n",
      "         0.0329,  0.0381,  0.3142,  0.3411,  0.2965,  0.1188,  0.3314,  0.2207,\n",
      "         0.0062,  0.2105,  0.2561,  0.0751, -0.1163,  0.1275,  0.1045,  0.2170,\n",
      "         0.1569,  0.2210, -0.1622,  0.2537, -0.0089,  0.2090,  0.0815,  0.0958,\n",
      "         0.0249, -0.0771, -0.1127,  0.0184,  0.3294,  0.1306,  0.2556,  0.0116,\n",
      "         0.2529,  0.0819,  0.1269, -0.0214,  0.1334,  0.2759,  0.1910,  0.0175,\n",
      "         0.2413, -0.0793, -0.3074,  0.1405, -0.2645,  0.0602, -0.0147,  0.0947,\n",
      "        -0.1469, -0.0464,  0.2732,  0.0361,  0.3827,  0.0962, -0.3585,  0.1156,\n",
      "        -0.2740,  0.2394, -0.0800,  0.0771,  0.0859, -0.1813, -0.0280, -0.1711,\n",
      "         0.3590,  0.0789,  0.0834,  0.1450,  0.3100,  0.3174,  0.3535, -0.1271,\n",
      "        -0.0801, -0.0006, -0.1620,  0.2876,  0.2147,  0.2633,  0.1358,  0.1645,\n",
      "         0.1344,  0.1203,  0.0378,  0.1743, -0.2376, -0.1963, -0.1045,  0.0751,\n",
      "         0.1705, -0.2398, -0.3148,  0.0248, -0.1797,  0.1733, -0.0023, -0.0469,\n",
      "         0.2113,  0.2400,  0.1340,  0.1627, -0.2370,  0.3683,  0.0972,  0.0575,\n",
      "         0.2804,  0.3166,  0.1179, -0.2762, -0.1362, -0.0068,  0.2512,  0.1123,\n",
      "         0.0392,  0.0029, -0.1281,  0.3030,  0.1666, -0.0589,  0.4311, -0.0866],\n",
      "       device='cuda:0')\n",
      "encoder.weight tensor([[-0.0741, -0.0578,  0.0632,  ...,  0.0594,  0.0566,  0.0760],\n",
      "        [-0.1522,  0.1169, -0.4883,  ..., -0.1272,  0.4498, -0.3550],\n",
      "        [ 0.2870,  0.1260,  0.4355,  ..., -0.0318,  0.1042,  0.2128],\n",
      "        ...,\n",
      "        [ 0.0201,  0.0821, -0.0665,  ...,  0.0639, -0.0060,  0.0507],\n",
      "        [-0.0849,  0.0226,  0.0130,  ...,  0.0454,  0.0955, -0.0490],\n",
      "        [ 0.0049, -0.0281,  0.0257,  ..., -0.0612, -0.0077, -0.0521]],\n",
      "       device='cuda:0')\n",
      "decoder.weight tensor([[ 0.0895, -0.0193,  0.0417,  ..., -0.0556, -0.0378, -0.0580],\n",
      "        [-0.0476, -0.2314, -0.1482,  ..., -0.0726, -0.0123,  0.1286],\n",
      "        [-0.1537, -0.2757,  0.2240,  ...,  0.0615, -0.2073,  0.0855],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0182, -0.0845,  ..., -0.0511, -0.0097,  0.0208],\n",
      "        [-0.0126, -0.0441, -0.0597,  ...,  0.0877,  0.0638, -0.0350],\n",
      "        [ 0.0561, -0.0604, -0.0041,  ..., -0.0064,  0.0411,  0.0220]],\n",
      "       device='cuda:0')\n",
      "decoder.bias tensor([-0.0220, 11.7777,  4.6457,  ..., -0.0160, -0.0144, -0.0157],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ntokens = 65292  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(type(state_dict))\n",
    "print(type(model))\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    print(key, value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.vocab.vocab.Vocab'>\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "file = open(\"vocab.obj\",'rb')\n",
    "vocab = pickle.load(file)\n",
    "file.close()\n",
    "input_text = \"This is a test sentence .\"\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "=\n",
      "type\n",
      "language\n",
      ".\n",
      "invalid\n",
      "/\n",
      "-\n",
      ",\n",
      "multiverseid\n",
      "format\n",
      "legality\n",
      "legal\n",
      "text\n",
      "card\n",
      "name\n",
      "the\n",
      "flavor\n",
      "{\n",
      "}\n",
      "com\n",
      "?\n",
      "wizards\n",
      "&\n",
      "http\n",
      "gatherer\n",
      "image\n",
      "ashx\n",
      "handlers\n",
      "a\n",
      "imageurl\n",
      "de\n",
      "1\n",
      "creature\n",
      "you\n",
      "—\n",
      "self\n",
      "of\n",
      "to\n",
      "'\n",
      "+\n",
      "it\n",
      ")\n",
      "(\n",
      "t\n",
      "date\n",
      "2\n",
      "la\n",
      "that\n",
      "if\n",
      "’\n",
      "s\n",
      "hand\n",
      "and\n",
      "or\n",
      "in\n",
      "your\n",
      "life\n",
      "criatura\n",
      "power\n",
      "number\n",
      "is\n",
      "que\n",
      "un\n",
      "toughness\n",
      "ability\n",
      "commander\n",
      "set\n",
      "as\n",
      "source\n",
      "colors\n",
      "3\n",
      "this\n",
      "r\n",
      "u\n",
      "names\n",
      "subtypes\n",
      "artist\n",
      "supertypes\n",
      "variations\n",
      "timeshifted\n",
      "on\n",
      "starter\n",
      "watermark\n",
      "rulings\n",
      "border\n",
      "cmc\n",
      "color_identity\n",
      "foreign_names\n",
      "id\n",
      "image_url\n",
      "layout\n",
      "legalities\n",
      "loyality\n",
      "mana_cost\n",
      "multiverse_id\n",
      "original_text\n",
      "original_type\n",
      "printings\n",
      "rarity\n",
      "release_date\n",
      "reserved\n",
      "set_name\n",
      "normal\n",
      "w\n",
      "target\n",
      "g\n",
      "b\n",
      "duel\n",
      "with\n",
      "vintage\n",
      "legacy\n",
      "battlefield\n",
      "an\n",
      "spell\n",
      "can\n",
      "control\n",
      "for\n",
      "mana\n",
      "turn\n",
      "di\n",
      "o\n",
      "player\n",
      "le\n",
      "its\n",
      "chinese\n",
      "una\n",
      "die\n",
      "du\n",
      "no\n",
      "créature\n",
      "from\n",
      "be\n",
      "cast\n",
      "modern\n",
      "creatura\n",
      "any\n",
      "put\n",
      "when\n",
      "kreatur\n",
      "cards\n",
      "japanese\n",
      "cost\n",
      "french\n",
      "german\n",
      "damage\n",
      "italian\n",
      "spanish\n",
      "penny\n",
      "do\n",
      "will\n",
      "4\n",
      "del\n",
      "x\n",
      "may\n",
      "el\n",
      "à\n",
      "brazil\n",
      "portuguese\n",
      "each\n",
      "e\n",
      "one\n",
      "der\n",
      "il\n",
      "creatures\n",
      "se\n",
      "paupercommander\n",
      "eine\n",
      "des\n",
      "turno\n",
      "at\n",
      "vous\n",
      "have\n",
      "um\n",
      "en\n",
      "0\n",
      "simplified\n",
      "has\n",
      "01\n",
      "are\n",
      "graveyard\n",
      "end\n",
      "all\n",
      "et\n",
      "until\n",
      "enters\n",
      "russian\n",
      "04\n",
      "into\n",
      "10\n",
      "whenever\n",
      "carta\n",
      "counter\n",
      "pioneer\n",
      "land\n",
      "face\n",
      "artifact\n",
      "by\n",
      "campo\n",
      "l\n",
      "les\n",
      "common\n",
      "permanent\n",
      "pauper\n",
      "une\n",
      "not\n",
      "other\n",
      "instant\n",
      "und\n",
      "abilities\n",
      "only\n",
      "sorcery\n",
      "choose\n",
      "deiner\n",
      "você\n",
      "human\n",
      "counters\n",
      "two\n",
      "06\n",
      "2021\n",
      "time\n",
      "si\n",
      "da\n",
      "traditional\n",
      "historic\n",
      "won\n",
      "rare\n",
      "uncommon\n",
      "che\n",
      "gladiator\n",
      "library\n",
      "ou\n",
      "then\n",
      "historicbrawl\n",
      "pay\n",
      "carte\n",
      "flying\n",
      "doesn\n",
      "exile\n",
      "opponent\n",
      "aura\n",
      "5\n",
      "wenn\n",
      "quando\n",
      "effect\n",
      "legendary\n",
      "con\n",
      "combat\n",
      "al\n",
      "d\n",
      "more\n",
      "sacrifice\n",
      "trigger\n",
      "they\n",
      "korean\n",
      "y\n",
      "votre\n",
      "draw\n",
      "enchantment\n",
      "para\n",
      "tu\n",
      "auf\n",
      "zu\n",
      "non\n",
      "explorer\n",
      "wahl\n",
      "09\n",
      "uma\n",
      "los\n",
      "gets\n",
      "token\n",
      "deals\n",
      "resolves\n",
      "i\n",
      "07\n",
      "up\n",
      "sur\n",
      "seu\n",
      "su\n",
      "their\n",
      "bersaglio\n",
      "copy\n",
      "karte\n",
      "objetivo\n",
      "2020\n",
      "ins\n",
      "alvo\n",
      "«\n",
      "»\n",
      "final\n",
      "first\n",
      "was\n",
      "than\n",
      "spiel\n",
      "premodern\n",
      "02\n",
      "color\n",
      "game\n",
      "esta\n",
      "planeswalker\n",
      "tensor([  72,   61,   29, 7200, 5106,    4])\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,300):\n",
    "    print(vocab.lookup_token(x))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenized_text = tokenizer(input_text)\n",
    "input_ids = torch.tensor(vocab(tokenized_text), dtype=torch.long)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "=\n",
      "type\n",
      "language\n",
      ".\n",
      "invalid\n",
      "/\n",
      "-\n",
      ",\n",
      "multiverseid\n",
      "format\n",
      "legality\n",
      "legal\n",
      "text\n",
      "card\n",
      "name\n",
      "the\n",
      "flavor\n",
      "{\n",
      "}\n",
      "com\n",
      "?\n",
      "wizards\n",
      "&\n",
      "http\n",
      "gatherer\n",
      "image\n",
      "ashx\n",
      "handlers\n",
      "a\n",
      "imageurl\n",
      "de\n",
      "1\n",
      "creature\n",
      "you\n",
      "—\n",
      "self\n",
      "of\n",
      "to\n",
      "'\n",
      "+\n",
      "it\n",
      ")\n",
      "(\n",
      "t\n",
      "date\n",
      "2\n",
      "la\n",
      "that\n",
      "if\n",
      "’\n",
      "s\n",
      "hand\n",
      "and\n",
      "or\n",
      "in\n",
      "your\n",
      "life\n",
      "criatura\n",
      "power\n",
      "number\n",
      "is\n",
      "que\n",
      "un\n",
      "toughness\n",
      "ability\n",
      "commander\n",
      "set\n",
      "as\n",
      "source\n",
      "colors\n",
      "3\n",
      "this\n",
      "r\n",
      "u\n",
      "names\n",
      "subtypes\n",
      "artist\n",
      "supertypes\n",
      "variations\n",
      "timeshifted\n",
      "on\n",
      "starter\n",
      "watermark\n",
      "rulings\n",
      "border\n",
      "cmc\n",
      "color_identity\n",
      "foreign_names\n",
      "id\n",
      "image_url\n",
      "layout\n",
      "legalities\n",
      "loyality\n",
      "mana_cost\n",
      "multiverse_id\n",
      "original_text\n",
      "original_type\n",
      "printings\n",
      "rarity\n",
      "release_date\n",
      "reserved\n",
      "set_name\n",
      "normal\n",
      "w\n",
      "target\n",
      "g\n",
      "b\n",
      "duel\n",
      "with\n",
      "vintage\n",
      "legacy\n",
      "battlefield\n",
      "an\n",
      "spell\n",
      "can\n",
      "control\n",
      "for\n",
      "mana\n",
      "turn\n",
      "di\n",
      "o\n",
      "player\n",
      "le\n",
      "its\n",
      "chinese\n",
      "una\n",
      "die\n",
      "du\n",
      "no\n",
      "créature\n",
      "from\n",
      "be\n",
      "cast\n",
      "modern\n",
      "creatura\n",
      "any\n",
      "put\n",
      "when\n",
      "kreatur\n",
      "cards\n",
      "japanese\n",
      "cost\n",
      "french\n",
      "german\n",
      "damage\n",
      "italian\n",
      "spanish\n",
      "penny\n",
      "do\n",
      "will\n",
      "4\n",
      "del\n",
      "x\n",
      "may\n",
      "el\n",
      "à\n",
      "brazil\n",
      "portuguese\n",
      "each\n",
      "e\n",
      "one\n",
      "der\n",
      "il\n",
      "creatures\n",
      "se\n",
      "paupercommander\n",
      "eine\n",
      "des\n",
      "turno\n",
      "at\n",
      "vous\n",
      "have\n",
      "um\n",
      "en\n",
      "0\n",
      "simplified\n",
      "has\n",
      "01\n",
      "are\n",
      "graveyard\n",
      "end\n",
      "all\n",
      "et\n",
      "until\n",
      "enters\n",
      "russian\n",
      "04\n",
      "into\n",
      "10\n",
      "whenever\n",
      "carta\n",
      "counter\n",
      "pioneer\n",
      "land\n",
      "face\n",
      "artifact\n",
      "by\n",
      "campo\n",
      "l\n",
      "les\n",
      "common\n",
      "permanent\n",
      "pauper\n",
      "une\n",
      "not\n",
      "other\n",
      "instant\n",
      "und\n",
      "abilities\n",
      "only\n",
      "sorcery\n",
      "choose\n",
      "deiner\n",
      "você\n",
      "human\n",
      "counters\n",
      "two\n",
      "06\n",
      "2021\n",
      "time\n",
      "si\n",
      "da\n",
      "traditional\n",
      "historic\n",
      "won\n",
      "rare\n",
      "uncommon\n",
      "che\n",
      "gladiator\n",
      "library\n",
      "ou\n",
      "then\n",
      "historicbrawl\n",
      "pay\n",
      "carte\n",
      "flying\n",
      "doesn\n",
      "exile\n",
      "opponent\n",
      "aura\n",
      "5\n",
      "wenn\n",
      "quando\n",
      "effect\n",
      "legendary\n",
      "con\n",
      "combat\n",
      "al\n",
      "d\n",
      "more\n",
      "sacrifice\n",
      "trigger\n",
      "they\n",
      "korean\n",
      "y\n",
      "votre\n",
      "draw\n",
      "enchantment\n",
      "para\n",
      "tu\n",
      "auf\n",
      "zu\n",
      "non\n",
      "explorer\n",
      "wahl\n",
      "09\n",
      "uma\n",
      "los\n",
      "gets\n",
      "token\n",
      "deals\n",
      "resolves\n",
      "i\n",
      "07\n",
      "up\n",
      "sur\n",
      "seu\n",
      "su\n",
      "their\n",
      "bersaglio\n",
      "copy\n",
      "karte\n",
      "objetivo\n",
      "2020\n",
      "ins\n",
      "alvo\n",
      "«\n",
      "»\n",
      "final\n",
      "first\n",
      "was\n",
      "than\n",
      "spiel\n",
      "premodern\n",
      "02\n",
      "color\n",
      "game\n",
      "esta\n",
      "planeswalker\n",
      "tensor([  72,   61,   29, 7200, 5106,    4])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(np\u001b[39m.\u001b[39marray(input_ids)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     38\u001b[0m \u001b[39m# get the output of your neural network\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m outputs \u001b[39m=\u001b[39m net(input_ids, attention_mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m, in \u001b[0;36mMyNet.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# encode the input using the transformer model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     23\u001b[0m     embeddings \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     25\u001b[0m     \u001b[39m# pass the embeddings through your neural network layers\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.encoder = model.encoder\n",
    "        self.fc1 = torch.nn.Linear(768, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # encode the input using the transformer model\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # pass the embeddings through your neural network layers\n",
    "        x = self.fc1(embeddings)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize your neural network\n",
    "net = MyNet()\n",
    "\n",
    "# preprocess your input data\n",
    "attention_mask = torch.ones(np.array(input_ids).shape)\n",
    "\n",
    "# get the output of your neural network\n",
    "outputs = net(input_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc816d8e426fe616fe65e3e533b3a5ac980db8a3cd9a2aa4e549e9c05f63913d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
